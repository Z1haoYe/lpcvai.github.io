<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>2021 Competition</title>
    <meta name="description" content>
    <meta name="keywords" content>

    <!-- Favicons -->
    <link href="../assets/img/favicon.png" rel="icon">
    <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com" rel="preconnect">
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="../assets/vendor/bootstrap/css/bootstrap.min.css"
      rel="stylesheet">
    <link href="../assets/vendor/bootstrap-icons/bootstrap-icons.css"
      rel="stylesheet">
    <link href="../assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="../assets/vendor/glightbox/css/glightbox.min.css"
      rel="stylesheet">
    <link href="../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Main CSS File -->
    <link href="../assets/css/main.css" rel="stylesheet">

    <!-- =======================================================
  * Template Name: BizPage
  * Template URL: https://bootstrapmade.com/bizpage-bootstrap-business-template/
  * Updated: Aug 07 2024 with Bootstrap v5.3.3
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
  </head>

  <body class="starter-page-page">

    <header id="header"
      class="header d-flex align-items-center fixed-top bg-dark ">
      <div
        class="container-fluid container-xl position-relative d-flex align-items-center justify-content-between">

        <a href="../index.html" class="logo d-flex align-items-center">
          <!-- Uncomment the line below if you also wish to use an image logo -->
          <!-- <img src="assets/img/logo.png" alt=""> -->
          <h1 class="sitename">LPCV</h1>
        </a>

        <nav id="navmenu" class="navmenu">
          <ul>
            <li><a href="../index.html">Home</a></li>
            <li class="dropdown"><a href="#"><span>History</span>
                <i
                  class="bi bi-chevron-down toggle-dropdown"></i></a>
              <ul>
                <li><a href="../history.html">Leaderboard</a></li>
                <li><a href="../competitions/2023.html" class="active">Past
                    Workshops</a></li>
              </ul>
            </li>
            <li><a href="../publications.html">Publications</a></li>

            <li class="dropdown"><a href="#"><span>LPCVC2025</span> <i
                  class="bi bi-chevron-down toggle-dropdown"></i></a>
              <ul>
                <li><a href="https://google.com">Introduction</a></li>
                <li><a href="#">Track 1</a></li>
                <li><a href="#">Track 2</a></li>
                <li><a href="#">Track 3</a></li>
              </ul>
            </li>
          </ul>
          <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
        </nav>

      </div>
    </header>

    <main class="main">

      <!-- Starter Section Section -->
      <div style="margin-top: 12%;">
        <div class="container" data-aos="fade-up">

          <form id="filter" style="display: flex; justify-content: center;">
            <h3>
              <abbr
                title="Low-Power Computer Vision Challenge">LPCVC</abbr>&nbsp;

              <select name="contest"
                onchange='location.replace(document.forms.filter.contest.value+".html");'>
                <!-- <select name="contest" id> -->
                <option
                  value="2023">2023</option><option
                  value="2022">2022</option><option
                  value="2021">2021</option><option
                  value="2020">2020</option><option
                  value="2019" selected>2019</option><option
                  value="2018">2018</option><option
                  value="2017">2017</option><option
                  value="2016">2016</option><option
                  value="2015">2015</option>
              </select>
            </h3>
          </form>

        </div>
      </div>

      <div data-aos="fade-up" class="card" style="margin: 5%; padding: 5%;">
        <div class="card"><div class="card-body">
            Three different competitions were run by the LPCV organizing
            committee. One
            was held <a href="#online">online</a> from November 2019 through
            January
            2020. Another was held at the <a href="#iccv">2019 International
              Conference
              on Computer Vision (ICCV) Workshop</a> on October 28, 2019 at
            Seoul. The
            earliest one was held at the <a href="#cvpr">Computer Vision and
              Pattern
              Recognition Conference (CVPR 2019)</a> on June 17, 2019 at Long
            Beach,
            CA.<br>
            At this period of time, the contest's name changed from LPIRC to
            LPCVC to
            include challenges in computer vision as a whole as opposed to just
            image
            recognition.
          </div></div>
        <p style="margin:0px">&nbsp;</p>
        <div class="card" id="online"><div class="card-body">
            <h1 style="text-align:center; border-bottom:1px dotted;">2019 IEEE
              Low-Power
              Image Recognition Challenge (LPIRC) Online Track</h1>
            <p>The 2019 IEEE Low-Power Image Recognition Challenge (LPIRC)
              concluded
              successfully on
              2020/01/31 . Seven teams participated and submitted 163 solutions.
              This
              challenge has two
              different tracks: object detection and image classification. The
              winners’
              solutions must
              outperform the solutions in earlier competitions.</p>
            <h3>Object Detection Track:</h3>
            <table class="table table-bordered">
              <thead>
                <th>Prize</th>
                <th>Team</th>
                <th>Member</th>
                <th>Representative</th>
              </thead>
              <tbody>
                <tr>
                  <td>First</td>
                  <td>MIT HAN Lab &amp; Dawnlight</td>
                  <td><ol>
                      <li>Tianzhe Wang</li>
                      <li>Han Cai</li>
                      <li>Shuai Zheng</li>
                      <li>Jia Li</li>
                      <li>Song Han</li>
                    </ol></td>
                  <td>Tianzhe Wang (<a
                      href="usedtobe@mit.edu">usedtobe@mit.edu</a>)</td>
                </tr>
                <tr>
                  <td>Second</td>
                  <td>Orange-Control (DiDi AI Labs)</td>
                  <td><ol>
                      <li>Chengxiang Yin (<a
                          href="mailto:cyin02@syr.edu">cyin02@syr.edu</a>)</li>
                      <li>Yue Shi (<a
                          href="mailto:dishenshiyue_i@didiglobal.com">dishenshiyue_i@didiglobal.com</a>)</li>
                      <li>Zhengping Che (<a
                          href="mailto:chezhengping@didiglobal.com">chezhengping@didiglobal.com</a>)</li>
                      <li>Ning Liu (<a
                          href="mailto:neilliuning@didiglobal.com">neilliuning@didiglobal.com</a>)</li>
                      <li>Kun Wu (<a
                          href="mailto:kwu102@syr.edu">kwu102@syr.edu</a>)</li>
                      <li>Xuefeng Shi (<a
                          href="mailto:shixuefeng@didiglobal.com">shixuefeng@didiglobal.com</a>)</li>
                      <li>Jian Tang (<a
                          href="mailto:tangjian@didiglobal.com">tangjian@didiglobal.com</a>)</li>
                    </ol></td>
                  <td>Yue Shi (<a
                      href="mailto:dishenshiyue_i@didiglobal.com">dishenshiyue_i@didiglobal.com</a>)</td>
                </tr>
              </tbody>
            </table>
            <h3>Image Classification Track:</h3>
            <table class="table table-bordered">
              <thead>
                <th>Prize</th>
                <th>Team</th>
                <th>Member</th>
                <th>Representative</th>
              </thead>
              <tbody>
                <tr>
                  <td>First</td>
                  <td>MIT HAN Lab &amp; Dawnlight</td>
                  <td><ol>
                      <li>Tianzhe Wang</li>
                      <li>Han Cai</li>
                      <li>Shuai Zheng</li>
                      <li>Jia Li</li>
                      <li>Song Han</li>
                    </ol></td>
                  <td>Tianzhe Wang (<a
                      href="usedtobe@mit.edu">usedtobe@mit.edu</a>)</td>
                </tr>
                <tr>
                  <td>Second</td>
                  <td>EdgeAI, Qualcomm Canada</td>
                  <td><ol>
                      <li>Chen Feng</li>
                      <li>Jay Zhuo</li>
                      <li>Parker Zhang</li>
                      <li>Liang Shen</li>
                      <li>Tommy Chen</li>
                      <li>Yicheng Lin</li>
                    </ol></td>
                  <td>Chen Feng (<a
                      href="mailto:chenf@qti.qualcomm.com">chenf@qti.qualcomm.com</a>)</td>
                </tr>
              </tbody>
            </table>
            <p>These teams’ solutions outperform the best solutions in
              literature. The
              winning classification
              model is 3% above the Pareto frontier of previous solutions
              (including
              baseline models like
              MobileNetV2, MnasNet, etc), while the best detection model is 5.6
              mAP
              above previous
              solutions.</p>
            <p>LPIRC started in 2015 with the aim to identify the technologies
              for
              computer vision using energy
              efficiently. IEEE Rebooting Computing has been the primary
              financial
              sponsor and provides
              administrative support. In 2019, sponsors are Google, Mediatek,
              Xilinx,
              and IEEE Circuits and
              Systems Society. Past sponsors include Facebook, Nvidia, IEEE
              GreenICT,
              IEEE Council of
              Design Automation, IEEE Council on Superconductivity. The 2019
              LPIRC is
              organized by
              Purdue University and Duke University. Google provides a unified
              latency
              metric and an
              evaluation platform for the online track (also called On-Device
              Visual
              Intelligence Challenge).</p>
            <p>In 2020, LPIRC is renamed to Low-Power Computer Vision Challenge
              (LPCVC).
              A new track is
              added for processing video captured by UAV (unmanned aerial
              vehicle). The
              2020 LPCVC
              sponsors are Facebook, Xilinx, ELAN Microelectronics, Google, IEEE
              Rebooting Computing,
              IEEE Council of Electronic Design Automation, IEEE Circuits and
              Systems
              Societies. A
              workshop will be held on June 15, 2020 in Seattle (co-located with
              CVPR).
              More information is
              available <a
                href="https://lpcv.ai/2020CVPR/introduction">here</a>.</p>
          </div></div>
        <p style="margin:0px">&nbsp;</p>
        <div class="card" id="iccv"><div class="card-body">
            <article class="col-md-12">
              <div itemprop="articleBody">
                <h1
                  style="text-align:center; border-bottom:1px dotted;">Low-Power
                  Computer Vision Workshop at ICCV 2019</h1>
                <p style="text-align:center;"><a
                    href="http://iccv2019.thecvf.com/"><img
                      src="https://rebootingcomputing.ieee.org/images/files/images/iccv-2019.jpg"
                      alt="ICCV 2019" width="450" height="255"></a></p>
                <p style="text-align:center;">2019 ICCV Workshop<br>
                  Seoul, Republic of Korea<br>
                  Monday, 28 October 2019</p>
                <p>The 2019 IEEE Workshop on Low-Power Computer Vision concluded
                  successfully on 28 October 2019 in Seoul. This workshop was
                  co-located
                  with the International Conference on Computer Vision.</p>
                <p>This workshop features the winners of the past Low-Power
                  Image
                  Recognition Challenges, invited speakers from academia and
                  industry,
                  as well as presentations of accepted papers. Two tracks of
                  on-site
                  competitions were held. Google and Xilinx are the technical
                  and
                  financial sponsors of the on-site competitions.</p>
                <p>In the FPGA (Field Programmable Gate Arrays) Track, the
                  winners
                  are:</p>
                <div class="mobile-table">
                  <table class="table table-hover table-striped">
                    <tbody>
                      <tr>
                        <th width="20%">Rank</th>
                        <th width="20%">Team Name</th>
                        <th width="40%">Members</th>
                        <th width="20%">Emails</th>
                      </tr>
                      <tr>
                        <td>1</td>
                        <td>IPIU</td>
                        <td>Lian Yanchao, Jia Meixia, Gao Yanjie</td>
                        <td><a
                            href="mailto:xuliu@xidian.edu.cn">xuliu@xidian.edu.cn</a></td>
                      </tr>
                      <tr>
                        <td>2</td>
                        <td>SkrSkr</td>
                        <td>Weixiong Jiang, Yingjie Zhang, Yajun Ha</td>
                        <td><a
                            href="mailto:jiangwx@shanghaitech.edu.cn">jiangwx@shanghaitech.edu.cn</a></td>
                      </tr>
                      <tr>
                        <td>3</td>
                        <td>XJTU-Tripler</td>
                        <td>Boran Zhao, Pengchen Zong, Wenzhe Zhao</td>
                        <td><a
                            href="mailto:boranzhao@stu.xjtu.edu.cn">boranzhao@stu.xjtu.edu.cn</a><br>
                          <a
                            href="mailto:pengjuren@xjtu.edu.cn">pengjuren@xjtu.edu.cn</a></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p>In the DSP (Digital Signal Processing) Track, the winners
                  are:</p>
                <div class="mobile-table">
                  <table class="table table-hover table-striped">
                    <tbody>
                      <tr>
                        <th width="20%">Rank</th>
                        <th width="20%">Team Name</th>
                        <th width="40%">Members</th>
                        <th width="20%">Emails</th>
                      </tr>
                      <tr>
                        <td>1</td>
                        <td>MIT HAN Lab</td>
                        <td>Tianzhe Wang, Han Cai, Zhekai Zhang, Song Han</td>
                        <td><a
                            href="mailto:usedtobe@mit.edu">usedtobe@mit.edu</a><br>
                          <a href="mailto:hancai@mit.edu">hancai@mit.edu</a><br>
                          <a
                            href="mailto:zhangzk@mit.edu">zhangzk@mit.edu</a><br>
                          <a href="mailto:songhan@mit.edu">songhan@mit.edu</a>
                        </td>
                      </tr>
                      <tr>
                        <td>2</td>
                        <td>ForeverDuke</td>
                        <td>Hsin-Pai Cheng, Tunhou Zhang, Shiyu Li</td>
                        <td><a
                            href="mailto:dave.cheng@duke.edu">dave.cheng@duke.edu</a><br>
                          <a
                            href="mailto:tunhou.zhang@duke.edu">tunhou.zhang@duke.edu</a><br>
                          <a
                            href="mailto:shiyu.li@duke.edu">shiyu.li@duke.edu</a>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p>&nbsp;</p>
                <h2 style="border-bottom: 1px dotted;">2019 ICCV Workshop
                  Gallery</h2>
                <div style="text-align: center;"><a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_083129.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_083129.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_084436.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_084436.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_084936.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_084936.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_101833.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_101833.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_101941.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_101941.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_104043.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_104043.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_110718.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_110718.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_114115.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_114115.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_121141.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_121141.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_145716.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_145716.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_153714.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_153714.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_155415.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_155415.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_161338.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_161338.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_170638.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_170638.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173201.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173201.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173216.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173216.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173512.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173512.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173728.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_173728.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_174939.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_174939.jpg"
                      alt="ICCV photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_203730.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/iccv/20191028_203730.jpg"
                      alt="ICCV photo" width="250" height="188"></a></div>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Program for 2019 ICCV
                  Workshop
                  “Low-Power Computer Vision”</h2>
                <p><strong>28 October 2019, Seoul</strong></p>
                <div class="mobile-table">
                  <table class="table table-hover table-striped">
                    <tbody>
                      <tr>
                        <th width="10%;">Time</th>
                        <th width="90%;">Speaker</th>
                      </tr>
                      <tr>
                        <td>08:30</td>
                        <td>Organizers’ Welcome<br>
                          Yung-Hsiang Lu &lt;<a
                            href="mailto:yunglu@purdue.edu">yunglu@purdue.edu</a>&gt;
                          and<br>
                          Terence Martinez &lt;<a
                            href="mailto:t.c.martinez@ieee.org">t.c.martinez@ieee.org</a>&gt;</td>
                      </tr>
                      <tr>
                        <td>08:40</td>
                        <td>Tensorflow Online Competition 2018-2019, Bo Chen
                          &lt;<a
                            href="mailto:bochen@google.com">bochen@google.com</a>&gt;</td>
                      </tr>
                      <tr>
                        <td>08:50</td>
                        <td>2018 LPIRC Winners, Moderator: Peter Vajda &lt;<a
                            href="mailto:vajdap@fb.com">vajdap@fb.com</a>&gt;<br>
                          <ol>
                            <li>Algorithm-Software-Hardware Codesign for CNN
                              Acceleration, Soonhoi Ha &lt;<a
                                href="mailto:sha@snu.ac.kr">sha@snu.ac.kr</a>&gt;</li>
                            <li>The winning solution of LPIRC2018 track3 and its
                              applications to real world, Suwoong Lee &lt;<a
                                href="mailto:suwoong@etri.re.kr">suwoong@etri.re.kr</a>&gt;
                              and Bayanmunkh Odgerel &lt;<a
                                href="mailto:baynaamn7@kpst.co.kr">baynaamn7@kpst.co.kr</a>&gt;</li>
                          </ol></td>
                      </tr>
                      <tr>
                        <td>09:30</td>
                        <td>2019 LPIRC Winners, Moderator: Soonhoi Ha &lt;<a
                            href="mailto:sha@snu.ac.kr">sha@snu.ac.kr</a>&gt;<br>
                          <ol>
                            <li>Winning Solution on LPIRC-2019 Competition,
                              Hensen Chen
                              &lt;<a
                                href="mailto:hesen.chs@alibaba-inc.com">hesen.chs@alibaba-inc.com</a>&gt;</li>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_ji-lin.pdf"
                                target="_blank">On-Device Image Classification
                                with
                                Proxyless Neural Architecture Search and
                                Quantization-Aware Fine-tuning</a> (PDF, 15 MB),
                              Ji Lin
                              &lt;<a
                                href="mailto:jilin@mit.edu">jilin@mit.edu</a>&gt;</li>
                          </ol></td>
                      </tr>
                      <tr>
                        <td>10:10</td>
                        <td>Break</td>
                      </tr>
                      <tr>
                        <td>10:40</td>
                        <td>2018 and 2019 LPIRC Winner, Moderator: Bo Chen
                          &lt;<a
                            href="mailto:bochen@google.com">bochen@google.com</a>&gt;<br>
                          Award-winning Methods for Object Detection Challenge
                          at LPIRC
                          2019, Tao Sheng &lt;<a
                            href="mailto:tsheng@amazon.com">tsheng@amazon.com</a>&gt;,
                          Peng Lei &lt;<a
                            href="mailto:leipeng@amazon.com">leipeng@amazon.com</a>&gt;,
                          and Yang Liu &lt;<a
                            href="mailto:yangliu@lab126.com">yangliu@lab126.com</a>&gt;</td>
                      </tr>
                      <tr>
                        <td>11:10</td>
                        <td>New Perspectives, Moderator: Tao Sheng &lt;<a
                            href="mailto:tsheng@amazon.com">tsheng@amazon.com</a>&gt;<br>
                          <ol>
                            <li>Value-Based Deep Learning Hardware Acceleration,
                              Andreas
                              Moshovos &lt;<a
                                href="mailto:moshovos@eecg.toronto.edu">moshovos@eecg.toronto.edu</a>&gt;</li>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_edwin-park.pdf"
                                target="_blank">Ultra-low Power Always-On
                                Computer
                                Vision</a> (PDF, 1 MB), Edwin Park &lt;<a
                                href="mailto:epark@qti.qualcomm.com">epark@qti.qualcomm.com</a>&gt;</li>
                            <li>Hardware-Aware Deep Neural Architecture Search,
                              Peter
                              Vajda &lt;<a
                                href="mailto:vajdap@fb.com">vajdap@fb.com</a>&gt;</li>
                          </ol></td>
                      </tr>
                      <tr>
                        <td>12:40</td>
                        <td>Break</td>
                      </tr>
                      <tr>
                        <td>13:40</td>
                        <td>Paper Presentations, Moderator: Jaeyoun Kim &lt;<a
                            href="mailto:jaeyounkim@google.com">jaeyounkim@google.com</a>&gt;<br>
                          <ol>
                            <li>Direct Feedback Alignment based Convolutional
                              Neural
                              Network Training for Low-power Online Learning
                              Processor:
                              Donghyeon Han (KAIST); Hoi-Jun Yoo (KAIST).
                              Presented by
                              Donghyeon Han &lt;<a
                                href="mailto:hdh4797@kaist.ac.kr">hdh4797@kaist.ac.kr</a>&gt;.</li>
                            <li>A system-level solution for low-power object
                              detection:
                              Fanrong Li (Institute of Automation, Chinese
                              Academy of
                              Sciences); Zitao Mo (Institute of Automation,
                              Chinese
                              Academy of Sciences); Peisong Wang (Institute of
                              Automation, Chinese Academy of Sciences); Zejian
                              Liu
                              (Institute of Automation, Chinese Academy of
                              Sciences);
                              Jiayun Zhang (Institute of Automation, Chinese
                              Academy of
                              Sciences); Gang Li (Institute of Automation,
                              Chinese
                              Academy of Sciences); Qinghao Hu (Chinese Academy
                              of
                              Sciences); Xiangyu He (Institute of Automation,
                              Chinese
                              Academy of Sciences); Cong Leng (Institute of
                              Automation,Chinese Academy of Sciences; Nanjing
                              Artificial
                              Intelligence Chip Research, Institute of
                              Automation,
                              Chinese Academy of Sciences); Yang Zhang (AIRIA);
                              Jian
                              Cheng (Chinese Academy of Sciences, China).
                              Presented by
                              Fanrong Li &lt;<a
                                href="mailto:lifanrong2017@ia.ac.cn">lifanrong2017@ia.ac.cn</a>&gt;</li>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_gaetan-bahl.pdf"
                                target="_blank">Low-power neural networks for
                                semantic
                                segmentation of satellite images</a> (PDF, 6
                              MB): Gaetan
                              Bahl (INRIA); Florent Lafarge (INRIA); Lionel
                              Daniel (IRT
                              Saint-Exupery); Matthieu Moretti (IRT
                              Saint-Exupery).
                              Presented by Gaetan Bahl &lt;<a
                                href="mailto:gaetan.bahl@inria.fr">gaetan.bahl@inria.fr</a>&gt;</li>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_feiyang-zhu.pdf"
                                target="_blank">Efficient Single Image
                                Super-Resolution
                                via Hybrid Residual Feature Learning with
                                Compact
                                Back-Projection Network</a> (PDF, 2 MB): Qijun
                              Zhao
                              (Sichuan University); Feiyang Zhu (Sichuan
                              University).
                              Presented by Feiyang Zhu &lt;<a
                                href="mailto:2418288750@qq.com">2418288750@qq.com</a>&gt;</li>
                          </ol></td>
                      </tr>
                      <tr>
                        <td>15:00</td>
                        <td>Break</td>
                      </tr>
                      <tr>
                        <td>15:20</td>
                        <td>Paper Presentations, Moderator: Hsin-Pai Cheng
                          &lt;<a
                            href="mailto:dave.cheng@duke.edu">dave.cheng@duke.edu</a>&gt;<br>
                          <ol>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_julia-gusak.pdf"
                                target="_blank">Automated multi-stage
                                compression of
                                neural networks</a> (PDF, 444 KB): Julia Gusak
                              (Skolkovo
                              Institute of Science and Technology); Maksym
                              Kholyavchenko
                              (Innopolis University); Evgeny Ponomarev (Skolkovo
                              Institute of Science and Technology); Larisa
                              Markeeva
                              (Skolkovo Institute of Science and Technology);
                              Philip
                              Blagoveschensky (1. Skoltech; 2. Huawei Noah's Ark
                              Lab);
                              Andrzej Cichocki (Skolkovo Institute of Science
                              and
                              Technology); Ivan Oseledets (Skolkovo Institute of
                              Science
                              and Technology). Presented by Julia Gusak &lt;<a
                                href="mailto:y.gusak@skoltech.ru">y.gusak@skoltech.ru</a>&gt;</li>
                            <li>Enriching Variety of Layer-wise Learning
                              Information by
                              Gradient Combination: Chien-Yao Wang (Institute of
                              Information Science, Academia Sinica); Mark Liao
                              (Institute of Information Science, Academia
                              Sinica);
                              Ping-Yang Chen (National Taiwan Ocean University);
                              Jun-Wei
                              Hsieh (National Taiwan Ocean University).
                              Presented by
                              Chien-Yao Wang &lt;<a
                                href="mailto:x102432003@yahoo.com.tw">x102432003@yahoo.com.tw</a>&gt;</li>
                            <li>512KiB RAM is enough! Live camera face
                              re-identification
                              DNN on MCU: Maxim Zemlyanikin (Xperience AI);
                              Alexander
                              Smorkalov (Xperience AI); Grigory Serebryakov
                              (Xperience
                              AI); Anna Petrovicheva (Xperience AI); Tatiana
                              Khanova
                              (Xperience AI). Presented by Maxim Zemlyanikin
                              &lt;<a
                                href="mailto:maxim.zemlyanikin@xperience.ai">maxim.zemlyanikin@xperience.ai</a>&gt;</li>
                            <li><a
                                href="https://rebootingcomputing.ieee.org/images/files/pdf/iccv-2019_george-jose.pdf"
                                target="_blank">Real Time Object Detection On
                                Low Power
                                Embedded Platforms</a> (PDF, 2 MB): Aashish
                              Kumar
                              (Harman International India Pvt. Ltd.); George
                              Jose
                              (Harman International India Pvt. Ltd.); Srinivas
                              Kruthiventi S S (Harman International India Pvt.
                              Ltd.);
                              Sambuddha Saha (Harman International India Pvt.
                              Ltd);
                              Harikrishna Muralidhara (Harman International
                              India Pvt.
                              Ltd). Presented by George Jose &lt;<a
                                href="mailto:George.Jose@Harman.com">George.Jose@Harman.com</a>&gt;</li>
                          </ol></td>
                      </tr>
                      <tr>
                        <td>16:40</td>
                        <td>Break</td>
                      </tr>
                      <tr>
                        <td>16:50</td>
                        <td><a
                            href="https://rebootingcomputing.ieee.org/lpirc/2019/onsite-aia-track">On-Site
                            AI Acceleration Competition</a> (by invitation
                          only)<br>
                          <!-- Organizers:
      <ul>
      <li>Jaeyoun Kim &lt;<a href="mailto:jaeyounkim@google.com">jaeyounkim@google.com</a>&gt;</li>
      <li>Bo Chen &lt;<a href="mailto:bochen@google.com">bochen@google.com</a>&gt;</li>
      <li>Achille Brighton &lt;<a href="mailto:aib@google.com">aib@google.com</a>&gt;</li>
      <li>Paul Roberts &lt;<a href="mailto:pwroberts@google.com">pwroberts@google.com</a>&gt;</li>
      </ul> -->
                          Organizers: Google and Xilinx
                        </td>
                      </tr>
                      <tr>
                        <td>17:30</td>
                        <td>Adjourn</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Abstracts</h2>
                <p><strong>08:50-09:10</strong><br>
                  Title: Algorithm-Software-Hardware Codesign for CNN
                  Acceleration<br>
                  Speaker: Soonhoi Ha &lt;<a
                    href="mailto:sha@snu.ac.kr">sha@snu.ac.kr</a>&gt;</p>
                <p>Abstract: There are three approaches to improve the energy
                  efficiency
                  of computer vision algorithms based on convolution networks
                  (CNN). One
                  is to find an algorithm that gives better accuracy than the
                  other
                  known algorithms under the constraints of real-time
                  performance and
                  available resources. The second is to develop a new hardware
                  accelerator with higher performance per watt than GPUs that
                  are widely
                  used. Lastly, we can apply various software optimization
                  techniques
                  such as network pruning, quantization, and low rank
                  approximation for
                  a given network and the hardware platform. While those
                  approaches have
                  been researched separately, algorithm-software-hardware
                  codesign will
                  give the better result than any one alone. In this talk, I
                  will
                  present an effort that has been made in our laboratory. We
                  have
                  developed a novel CNN accelerator, based on the
                  hardware-software
                  codesign methodology where CNN compiler has been developed on
                  a
                  simulator without hardware implementation. We are developing
                  an NAS
                  (Neural architecture search) framework for the CNN
                  accelerator.</p>
                <p>Biography: Soonhoi Ha a professor and the chair in the
                  department of
                  Computer Science and Engineering at Seoul National University.
                  He
                  received the Ph.D. degree in EECS from U. C. Berkeley in 1992.
                  His
                  current research interests include HW/SW codesign of embedded
                  systems,
                  embedded machine learning, and IoT. He is an IEEE fellow.</p>
                <p>&nbsp;</p>
                <p><strong>09:10-09:30</strong><br>
                  Title: The winning solution of LPIRC 2018 track3 and its
                  applications
                  to real world<br>
                  Speaker: Suwoong Lee &lt;<a
                    href="mailto:suwoong@etri.re.kr">suwoong@etri.re.kr</a>&gt;
                  and
                  Bayanmunkh Odgerel &lt;<a
                    href="mailto:baynaamn7@kpst.co.kr">baynaamn7@kpst.co.kr</a>&gt;</p>
                <p>Abstract: In this talk, we review the winning solution of
                  LPIRC 2018
                  track3 and its practical applications. In the first part,
                  detailed
                  procedures for finding the winning solution are presented. We
                  also
                  present our approaches for improving the winning solution
                  after LPIRC
                  2018 challenge. In the second part, several practical
                  applications
                  using our low-power image recognition techniques are
                  introduced.
                  Applications are reviewed in three aspects: software,
                  hardware, and
                  platforms. Finally, we conclude our talk by presenting future
                  research
                  directions.</p>
                <p>Biography: Suwoong Lee is senior researcher at Electronics
                  and
                  Telecommunications Research Institute (ETRI). He received M.S
                  at
                  KAIST, and joined the creative contents research division at
                  ETRI in
                  2007. He has researched various image understanding
                  technologies and
                  developed related products, before and after the deep learning
                  era.
                  His current research interest is lightweight object
                  recognition
                  technologies using deep learning.</p>
                <p>The project’s contributors include Suwoong Lee &lt;<a
                    href="mailto:suwoong@etri.re.kr">suwoong@etri.re.kr</a>&gt;,
                  Bayanmunkh Odgerel &lt;<a
                    href="mailto:aynaamn7@kpst.co.kr">aynaamn7@kpst.co.kr</a>&gt;,
                  Seungjae Lee &lt;<a
                    href="mailto:seungjlee@etri.re.kr">seungjlee@etri.re.kr</a>&gt;,
                  Junhyuk Lee &lt;<a
                    href="mailto:jun@kpst.co.kr">jun@kpst.co.kr</a>&gt;, Hong
                  Hanh
                  Nguyen &lt;<a
                    href="mailto:honghanh@kpst.co.kr">honghanh@kpst.co.kr</a>&gt;,
                  Jong
                  Gook Ko &lt;<a
                    href="mailto:jgko@etri.re.kr">jgko@etri.re.kr</a>&gt;.</p>
                <p>Seungjae Lee is a principal researcher at the Electronics and
                  Telecommunications Research Institute(ETRI). He has researched
                  content
                  identification, classification, and retrieval systems. Junhyuk
                  Lee is
                  CEO at Korea Platform Service Technologies (KPST). He founded
                  KPST in
                  2008, and currently, he is leading deep learning and mobile
                  edge
                  computing R&amp;D team at KPST. Hong Hanh Nguyen is currently
                  working
                  as a senior member of the R&amp;D team of KPST. Her current
                  research
                  is focusing on object recognition solutions using deep
                  learning on
                  embedded devices. Jong Gook Ko is a principal researcher at
                  ETRI. He
                  joined the security research division at ETRI in 2000. His
                  current
                  research interest is object recognition technologies using
                  deep
                  learning. ETRI &amp; KPST team participated in visual
                  searching
                  related challenges such as ImageNet challenge (classification
                  and
                  localization: 5th place in 2016, detection: 3rd place in
                  2017), Google
                  Landmark Retrieval (8th place in 2018) and Low Power Image
                  Recognition
                  Challenge (1st place in 2018).</p>
                <p>&nbsp;</p>
                <p><strong>09:30-09:50</strong><br>
                  Title: Winning Solution on LPIRC-2019 Competition<br>
                  Speaker: Hensen Chen &lt;<a
                    href="mailto:hesen.chs@alibaba-inc.com">hesen.chs@alibaba-inc.com</a>&gt;</p>
                <p>Abstract: With the development of computer vision technology
                  in
                  recent years, more and more mobile/IoT devices rely on visual
                  data for
                  making decisions. The increasing industrial demands to deploy
                  deep
                  neural networks on resource constrained mobile device
                  motivates recent
                  research of efficient structure for deep learning. In this
                  work, we
                  designed a hardware-friendly deep neural network structure
                  based on
                  the characteristics of mobile devices. Combined with advanced
                  training
                  programs, our mode archive 8-bit inference top-1 accuracy in
                  66.93% on
                  imagenet validation dataset, with only 23ms inference speed on
                  Google
                  Pixel2.</p>
                <p>Abstract: Hensen is an algorithm engineer at Alibaba Group
                  Inc.
                  Hensen received Master degree in electronic engineering from
                  Beijing
                  University of Posts and Telecommunications in 2018 under the
                  supervision of Prof. Jingyu Wang. Hensen’s research interest
                  mainly
                  focuses on model compression and efficient convolutional
                  network.</p>
                <p>&nbsp;</p>
                <p><strong>09:50-10:10</strong><br>
                  Title: On-Device Image Classification with Proxyless Neural
                  Architecture Search and Quantization-Aware Fine-tuning<br>
                  Speaker: Ji Lin &lt;<a
                    href="mailto:jilin@mit.edu">jilin@mit.edu</a>&gt;</p>
                <p>Abstract: It is challenging to efficiently deploy deep
                  learning
                  models on resource-constrained hardware devices (e.g., mobile
                  and IoT
                  devices) with strict efficiency constraints (e.g., latency,
                  energy
                  consumption). We employ Proxyless Neural Architecture Search
                  (ProxylessNAS) to auto design compact and specialized neural
                  network
                  architectures for the target hardware platform. ProxylessNAS
                  makes
                  latency differentiable, so we can optimize not only accuracy
                  but also
                  latency by gradient descent. Such direct optimization saves
                  the search
                  cost by 200× compared to conventional neural architecture
                  search
                  methods. Our work is followed by quantization-aware
                  fine-tuning to
                  further boost efficiency. In the Low Power Image Recognition
                  Competition at CVPR 19, our solution won the 3rd place on the
                  task of
                  Real-Time Image Classification (online track).</p>
                <p>Biography: Ji is currently a first-year Ph.D. student at MIT
                  EECS.
                  Previously, He graduated from the Department of Electronic
                  Engineering, Tsinghua University. His research interests lie
                  in
                  efficient and hardware-friendly machine learning and its
                  applications</p>
                <p>&nbsp;</p>
                <p><strong>10:40-11:10</strong><br>
                  Title: Award-winning Methods for Object Detection Challenge at
                  LPIRC
                  2019<br>
                  Speaker: Tao Sheng &lt;<a
                    href="mailto:tsheng@amazon.com">tsheng@amazon.com</a>&gt;,
                  Peng Lei
                  &lt;<a
                    href="mailto:leipeng@amazon.com">leipeng@amazon.com</a>&gt;</p>
                <p>Abstract: The LPIRC is an annual competition for the best
                  technologies in image classification and object detection
                  measured by
                  both efficiency and accuracy. As the winners were announced at
                  and
                  LPIRC-II 2018 and LPIRC-CVPR 2019, our Amazon team has won the
                  top
                  prizes for object detection challenge and image classification
                  challenge. We would like to share our award-winning methods in
                  this
                  talk, which can be summarized as five major steps. First,
                  8-bit
                  quantization friendly model is one of the key winning points
                  to
                  achieve the fast execution time while maintaining the high
                  accuracy on
                  edge devices. Second, network architecture optimization is
                  another
                  winning keypoint. We optimized the network architecture to
                  meet the
                  latency requirement on Pixel2 phone. The third one is dataset
                  filtering. We removed the images with small objects from the
                  training
                  dataset after thoroughly analyzing the training curves, which
                  significantly improved the overall accuracy. The forth one is
                  the loss
                  function optimization. And the fifth one is non-maximum
                  suppression
                  optimization. By combining all the above steps, our final
                  solutions
                  were able to win the 1st prize of object detection challenge
                  in the
                  two consecutive LPIRC.</p>
                <p>Biography: Dr. Tao Sheng is a Senior Deep Learning Engineer
                  at
                  Amazon. He has been working on multiple cutting-edge projects
                  in the
                  research areas of computer vision, machine learning in more
                  than 10
                  years. Prior to Amazon, he worked with Qualcomm and Intel. He
                  has
                  strong interests in deep learning for mobile vision, edge AI,
                  etc. He
                  has published ten US and International patents and night
                  papers. Most
                  recently, he led the team to win the Top Prizes of IEEE
                  International
                  Low-Power Image Recognition Challenge at LPIRC-I 2018,
                  LPIRC-II 2018,
                  and LPIRC 2019. Dr. Peng Lei is currently an Applied Scientist
                  at
                  Amazon.com Services, Inc. His role is to apply advanced
                  Computer
                  Vision and Machine Learning techniques to build solutions to
                  challenging problems that directly impact the company's
                  product. He is
                  an experienced researcher with a demonstrated history of
                  working in
                  both academia and industry. His research interests include
                  object
                  detection, person re-identification, temporal action
                  segmentation,
                  semantic video segmentation and motion estimation in videos.
                  He
                  received a Ph.D. degree from Oregon State University in 2018
                  with a
                  successful record of publications including 2 CVPRs, 2 ECCVs,
                  1 ICCV,
                  2 ICPRs and 1 CVPR Workshop. He won 1st place in Low-Power
                  Image
                  Recognition Challenge 2019 and 4th place in DAVIS Video
                  Segmentation
                  Challenge 2017, and served as a reviewer for CVPR 2019, ICCV
                  2019 and
                  AAAI 2020.</p>
                <p>&nbsp;</p>
                <p><strong>11:10-11:40</strong><br>
                  Title: Value-Based Deep Learning Hardware Acceleration<br>
                  Speaker: Andreas Moshovos &lt;<a
                    href="mailto:moshovos@eecg.toronto.edu">moshovos@eecg.toronto.edu</a>&gt;</p>
                <p>Abstract: I will be reviewing our efforts in identifying
                  value
                  properties of Deep Learning models that hardware accelerators
                  can use
                  to improve execution time performance, memory traffic and
                  storage, and
                  energy efficiency. Our goal it to not sacrifice accuracy and
                  to not
                  require any changes to the model. I will be presenting our
                  accelerator
                  family which includes designs that exploit these properties.
                  Our
                  accelerators exploit ineffectual activations and weights,
                  their
                  variable precision requirements, or even their value content
                  at the
                  bit level. Further, our accelerators also enable on-the-fly
                  trading
                  off accuracy for further performance and energy efficiency
                  improvements. I will emphasize our work on accelerators for
                  machine
                  learning implementations of computational imaging. Finally, I
                  will
                  overview NSERC COHESA, a Canadian research network which
                  targets the
                  co-design of next generation machine learning hardware and
                  algorithms.</p>
                <p>Biography: Dr.Moshovos teaches at the Department of
                  Electrical and
                  Computer Engineering at the University of Toronto. He taught
                  at the
                  Northwestern University, USA, the University of Athens,
                  Greece, the
                  Hellenic Open University, Greece, and as a invited professor
                  at the
                  École polytechnique fédérale de Lausanne, Switzerland. He
                  received a
                  Bachelor and a Master Degree from the University of Crete,
                  Greece and
                  a Ph.D. from the University of Wisconsin-Madison.</p>
                <p>His research interests lie primarily in the design of
                  performance-,
                  energy-, and/or cost-optimized computing engines for various
                  applications domains. Most work thus far has been on
                  high-performance
                  general-purpose systems.My current work emphasizes
                  highly-specialized
                  computing engines for Deep Learning. He will also be serving
                  as the
                  Director of the newly formed National Sciences and Engineering
                  Research Council Strategic Partnership Network on Machine
                  Learning
                  Hardware Acceleration (NSERC COHESA), a partnership of 19
                  Researchers
                  across 7 Universities involving 8 Industrial Partners. He has
                  been
                  awarded the ACM SIGARCH Maurice Wilkes mid-career award, a
                  National
                  Research Foundation CAREER Award, two IBM Faculty Partnership
                  awards,
                  a Semiconductor Research Innovation award, an IEEE Top Picks
                  in
                  Computer Architecture Research, and a MICRO conference Hall of
                  Fame
                  award. He has served as the Program Chair for the ACM/IEEE
                  International Symposium on Microarchitecture and the ACM/IEEE
                  International Symposium on Performance Analysis of Systems and
                  Software. He is a Fellow of the ACM and a Faculty Affiliate of
                  the
                  Vector Institute.</p>
                <p>&nbsp;</p>
                <p><strong>11:40-12:10</strong><br>
                  Title: Ultra-low Power Always-On Computer Vision<br>
                  Speaker: Edwin Park &lt;<a
                    href="mailto:epark@qti.qualcomm.com">epark@qti.qualcomm.com</a>&gt;</p>
                <p>Abstract: The challenge of on device battery power computer
                  vision is
                  to process computer vision at ultra low power. In tis talk, we
                  will
                  discuss computer vision looking at the entire pipeline from
                  sensor to
                  results.</p>
                <p>Biography: Edwin Park is a Principal Engineer with Qualcomm
                  Research
                  since 2011. He is directly responsible for leading the system
                  activities surrounding the Always-on Computer Vision Module
                  (CVM)
                  project. In this role, he leads his team in developing the
                  architecture, creating the algorithms, and producing models so
                  devices
                  can “see” at the lowest power. Featuring an extraordinarily
                  small form
                  factor, Edwin has designed the CVM to be integrated into a
                  wide
                  variety of battery-powered and line-powered devices,
                  performing object
                  detection, feature recognition, change/motion detection, and
                  other
                  applications.</p>
                <p>Since joining Qualcomm Research, Edwin has worked on a
                  variety of
                  projects such as making software modifications to Qualcomm
                  radios,
                  enabling them to support different radio standards. Edwin has
                  also
                  focused on facilitating carriers’ efforts worldwide to migrate
                  from
                  their legacy 2G cellular systems to 3G, allowing them to
                  increase
                  their data capacity. Edwin Park’s work at Qualcomm Research
                  has
                  resulted in over 40 patents.</p>
                <p>Prior to joining Qualcomm, Park was a founder at AirHop
                  Communications and Vie Wireless Technologies. Edwin had
                  various
                  engineering and management positions at Texas Instruments and
                  Dot
                  Wireless. He also worked at various other startups including
                  ViaSat
                  and Nextwave.</p>
                <p>Park received a Master Electrical Engineering from Rice
                  University,
                  Houston, TX, USA and a BSEE specializing in Physical
                  Electronics and
                  BA in Economics also from Rice University.</p>
                <p>&nbsp;</p>
                <p><strong>12:10-12:40</strong><br>
                  Title: Hardware-Aware Deep Neural Architecture Search<br>
                  Speaker: Peter Vajda &lt;<a
                    href="mailto:vajdap@fb.com">vajdap@fb.com</a>&gt;</p>
                <p>Abstract: A central problem in the deployment of deep neural
                  networks
                  is maximizing accuracy within the compute performance
                  constraints of
                  low power devices. In this talk, we will discuss approaches to
                  addressing this challenge based automated network search and
                  adaptation algorithms. These algorithms not only discover
                  neural
                  network models that surpass state-of-the-art accuracy, but are
                  also
                  able to adapt models to achieve efficient implementation on
                  low power
                  platforms for real-world applications.</p>
                <p>Biography: Peter Vajda is a Research Manager leading the
                  Mobile
                  Vision efforts at Facebook. Before joining Facebook, he was a
                  Visiting
                  Assistant Professor in Professor Bernd Girod’s group at
                  Stanford
                  University, Stanford, USA. He was working on personalized
                  multimedia
                  system and mobile visual search. Peter completed his Ph.D.
                  with Prof.
                  Touradj Ebrahimi at the Ecole Polytechnique Fédéral de
                  Lausanne
                  (EPFL), Lausanne, Switzerland, 2012.</p>
                <p>&nbsp;</p>
                <p><strong>13:40-14:00</strong><br>
                  Title: Direct Feedback Alignment based Convolutional Neural
                  Network
                  Training for Low-power Online Learning Processor<br>
                  Speaker: Donghyeon Han &lt;<a
                    href="mailto:hdh4797@kaist.ac.kr">hdh4797@kaist.ac.kr</a>&gt;<br>
                  Biography: Donghyeon Han received the B.S. and M.S. degree in
                  electrical engineering from the Korea Advanced Institute of
                  Science
                  and Technology (KAIST), Daejeon, South Korea, in 2017 and
                  2019,
                  respectively. He is currently pursuing the Ph.D. degree. His
                  current
                  research interests include low-power system-on-chip design,
                  especially
                  focused on deep neural network learning accelerators and
                  hardware-friendly deep learning algorithms.</p>
                <p>&nbsp;</p>
                <p><strong>14:00-14:20</strong><br>
                  Title: A system-level solution for low-power object
                  detection<br>
                  Speaker: Fanrong Li &lt;<a
                    href="mailto:lifanrong2017@ia.ac.cn">lifanrong2017@ia.ac.cn</a>&gt;<br>
                  Biography: Fanrong Li is a Ph.D. student at the Institute of
                  Automation, Chinese Academy of Science. His research interests
                  mainly
                  focus on analyzing and designing computer architectures for
                  neural
                  networks as well as developing heterogeneous reconfigurable
                  accelerators.</p>
                <p>&nbsp;</p>
                <p><strong>14:20-14:40</strong><br>
                  Title: Low-power neural networks for semantic segmentation of
                  satellite images<br>
                  Speaker: Gaetan Bahl &lt;<a
                    href="mailto:gaetan.bahl@inria.fr">gaetan.bahl@inria.fr</a>&gt;<br>
                  Biography: Gaetan is a first year PhD Student at INRIA (French
                  Institute for Research in Computer Science and Automation),
                  funded by
                  IRT Saint-Exupery and working on Deep Learning architectures
                  for
                  onboard satellite image analysis. After receiving his Master's
                  degree
                  from Grenoble INP - Ensimag in 2017, he worked in the
                  photogrammetry
                  industry for a year on digital surface model generation from
                  drone
                  imagery and deep learning for segmentation of 3D urban
                  models.</p>
                <p>&nbsp;</p>
                <p><strong>14:40-15:00</strong><br>
                  Title: Efficient Single Image Super-Resolution via Hybrid
                  Residual
                  Feature Learning with Compact Back-Projection Network<br>
                  Speaker: Feiyang Zhu &lt;<a
                    href="mailto:2418288750@qq.com">2418288750@qq.com</a>&gt;<br>
                  Biography: Feiyang is a graduate student at the College of
                  Computer
                  Science of Sichuan University.</p>
                <p>&nbsp;</p>
                <p><strong>15:20-15:40</strong><br>
                  Title: Automated multi-stage compression of neural
                  networks<br>
                  Speaker: Julia Gusak &lt;<a
                    href="mailto:y.gusak@skoltech.ru">y.gusak@skoltech.ru</a>&gt;<br>
                  Biography: Julia received her Master's degree in Mathematics
                  and Ph.D.
                  degree in Probability Theory and Statistics from Lomonosov
                  Moscow
                  State University. Now she is a postdoc at Skolkovo Institute
                  of
                  Science and Technology in the laboratory
                  "Tensor networks and deep learning for applications in data mining",
                  where she closely collaborates with Prof. Ivan Oseledets and
                  Prof.
                  Andrzej Cichocki. Her recent research deals with neural
                  networks
                  compression and acceleration. Also, she is very interested in
                  audio-related problems, in particular, dialogue systems, text
                  to
                  speech and speaker identification tasks.</p>
                <p>&nbsp;</p>
                <p><strong>15:40-16:00</strong><br>
                  Title: Enriching Variety of Layer-wise Learning Information by
                  Gradient Combination<br>
                  Speaker: Chien-Yao Wang &lt;<a
                    href="mailto:x102432003@yahoo.com.tw">x102432003@yahoo.com.tw</a>&gt;<br>
                  Biography: Chien-Yao Wang received the B.S. degree and PH. D.
                  degree
                  in applied computer science and information engineering from
                  National
                  Central University, Zhongli, Taiwan, in 2013 and 2017.
                  Currently, he
                  is a Postdoctoral Research Scholar in the Institute of
                  Information
                  Science, Academia Sinica. His research interests include
                  signal
                  processing, deep learning, and machine learning. He is an
                  honorary
                  member of Phi Tau Phi Scholastic Honor Society.</p>
                <p>&nbsp;</p>
                <p><strong>16:00-16:20</strong><br>
                  Title: 512KiB RAM is enough! Live camera face
                  re-identification DNN on
                  MCU<br>
                  Speaker: Maxim Zemlyanikin &lt;<a
                    href="mailto:maxim.zemlyanikin@xperience.ai">maxim.zemlyanikin@xperience.ai</a>&gt;<br>
                  Biography: Maxim is a deep learning engineer at Xperience AI.
                  His
                  research interests are image retrieval in various fields (face
                  recognition, person re-identification, visual search in
                  fashion
                  domain) and low-power deep learning (neural network
                  compression
                  techniques, resource-efficient neural network architectures).
                  Maxim
                  received his MSc in Data Mining from the Higher School of
                  Economics in
                  2019.</p>
                <p>&nbsp;</p>
                <p><strong>16:20-16:40</strong><br>
                  Title: Real Time Object Detection On Low Power Embedded
                  Platforms<br>
                  Speaker: George Jose &lt;<a
                    href="mailto:George.Jose@Harman.com">George.Jose@Harman.com</a>&gt;<br>
                  Biography: George Jose completed his gradation in Electronics
                  and
                  Communication Engineering from National Institute of
                  Technology,
                  Calicut. He then completed his post graduation at IIT Bombay
                  where he
                  worked on beamforming techniques to improve distant speech
                  recognition
                  using microphone arrays. Currently he is working as Assistant
                  Software
                  Engineer at Harman International. In his 2 years of experience
                  in the
                  software industry, he has worked on a wide range of areas like
                  object
                  detection for ADAS systems, autonomous indoor navigation,
                  adversarial
                  defence mechanisms, speaker identification, keyword detection
                  etc.</p>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Organizers</h2>
                <ul>
                  <li>Alexander C Berg, Associate Professor, University of North
                    Carolina at Chapel Hill, <a
                      href="mailto:aberg@cs.unc.edu">aberg@cs.unc.edu</a></li>
                  <li>Bo Chen, Software Engineer, Google Inc. <a
                      href="mailto:bochen@google.com">bochen@google.com</a></li>
                  <li>Yiran Chen, Associate Professor, Duke University, <a
                      href="mailto:yiran.chen@duke.edu">yiran.chen@duke.edu</a></li>
                  <li>Yen-Kuang Chen, Research Scientist, Alibaba, <a
                      href="mailto:y.k.chen@ieee.org">y.k.chen@ieee.org</a></li>
                  <li>Eui-Young Chung, Professor, Yonsei University, <a
                      href="mailto:eychung@yonsei.ac.kr">eychung@yonsei.ac.kr</a></li>
                  <li>Svetlana Lazebnik, Associate Professor, University of
                    Illinois, <a
                      href="mailto:slazebni@illinois.edu">slazebni@illinois.edu</a></li>
                  <li>(<strong>contact</strong>) Yung-Hsiang Lu, Professor,
                    Purdue
                    University, <a
                      href="mailto:yunglu@purdue.edu">yunglu@purdue.edu</a></li>
                  <li>Sungroh Yoon, Associate Professor, Seoul National
                    University, <a
                      href="mailto:sryoon@snu.ac.kr">sryoon@snu.ac.kr</a></li>
                </ul>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">References</h2>
                <ol>
                  <li>Y. Lu, A. M. Kadin, A. C. Berg, T. M. Conte, E. P.
                    DeBenedictis,
                    R. Garg, G. Gingade, B. Hoang, Y. Huang, B. Li, J. Liu, W.
                    Liu, H.
                    Mao, J. Peng, T. Tang, E. K. Track, J. Wang, T. Wang, Y.
                    Wang, and
                    J. Yao. Rebooting computing and low-power image recognition
                    challenge. In 2015 IEEE/ACM International Conference on
                    Computer-Aided Design (ICCAD), pages 927–932, Nov 2015.</li>
                  <li>K. Gauen, R. Rangan, A. Mohan, Y. Lu, W. Liu, and A. C.
                    Berg.
                    Low-power image recognition challenge. In 2017 22nd Asia and
                    South
                    Pacific Design Automation Conference (ASP-DAC), pages
                    99–104, Jan
                    2017.</li>
                  <li>K. Gauen, R. Dailey, Y. Lu, E. Park, W. Liu, A. C. Berg,
                    and Y.
                    Chen. Three years of low-power image recognition challenge:
                    Introduction to special session. In 2018 Design, Automation
                    Test in
                    Europe Conference Exhibition (DATE), pages 700–703, March
                    2018.</li>
                  <li>Sergei Alyamkin, Matthew Ardi, Achille Brighton, Alexander
                    C.
                    Berg, Yiran Chen, Hsin-Pai Cheng, Bo Chen, Zichen Fan, Chen
                    Feng, Bo
                    Fu, Kent Gauen, Jongkook Go, Alexander Goncharenko, Xuyang
                    Guo, Hong
                    Hanh Nguyen, Andrew Howard, Yuanjun Huang, Donghyun Kang,
                    Jaeyoun
                    Kim, Alexander Kondratyev, Seungjae Lee, Suwoong Lee,
                    Junhyeok Lee,
                    Zhiyu Liang, Xin Liu, Juzheng Liu, Zichao Li, Yang Lu,
                    Yung-Hsiang
                    Lu, Deeptanshu Malik, Eun-byung Park, Denis Repin, Tao
                    Sheng, Liang
                    Shen, Fei Sun, David Svitov, George K. Thiruvathukal, Baiwu
                    Zhang,
                    Jingchi Zhang, Xiaopeng Zhang, and Shaojie Zhuo. 2018
                    low-power
                    image recognition challenge. CoRR, abs/1810.01732,
                    2018.</li>
                  <li>Yung-Hsiang Lu, Alexander C. Berg, and Yiran Chen.
                    Low-power image
                    recognition challenge. AI Magazine, 39(2), Summer 2018.</li>
                </ol>
                <div class="clearfix"></div></div></article>
          </div></div>

        <p style="margin:0px">&nbsp;</p>

        <div id="cvpr" class="card"><div class="card-body">
            <article class="col-md-12">
              <div itemprop="articleBody">
                <!-- <div class="row">
        <div class="col-md-9"> -->
                <h1 style="text-align:center; border-bottom:1px dotted;">CVPR
                  Workshop
                  Low-Power Image Recognition Challenge (LPIRC)</h1>
                <p style="text-align:center;">Held as part of <a
                    href="http://cvpr2019.thecvf.com/">Computer Vision and
                    Pattern
                    Recognition Conference (CVPR 2019)</a></p>
                <p style="text-align:center;"><img
                    src="https://rebootingcomputing.ieee.org/images/files/images/long_beach.jpg"
                    alt="Long Beach, CA, USA"
                    class="img-responsive center-block"></p>
                <p style="text-align:center;">17 June 2019 | Long Beach,
                  California,
                  USA</p>
                <p style="text-align:center;"><strong>Recent
                    publication:</strong><br>
                  <a
                    href="https://ieeexplore.ieee.org/document/8693826">Low-Power
                    Computer Vision: Status, Challenges, and
                    Opportunities</a><br>
                  <em>IEEE Journal on Emerging and Selected Topics in Circuits
                    and
                    Systems</em> (Volume: 9, Issue: 2, June 2019)</p>
                <!-- </div>
      <div class="col-md-3">
      <p>&nbsp;</p>
      <p>&nbsp;</p>
      <p>Previous LPIRC Editions:</p>
      <ul>
      <li><a href="https://rebootingcomputing.ieee.org/lpirc/2018">LPIRC 2018</a></li>
      <li><a href="https://rebootingcomputing.ieee.org/lpirc/2017">LPIRC 2017</a></li>
      <li><a href="https://rebootingcomputing.ieee.org/lpirc/2016">LPIRC 2016</a></li>
      <li><a href="https://rebootingcomputing.ieee.org/lpirc/2015">LPIRC 2015</a></li>
      </ul>
      </div>
      </div> -->
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Winning Teams from LPIRC
                  2019</h2>
                <p>The 2019 IEEE Low-Power Image Recognition Challenge (LPIRC)
                  concluded
                  successfully on 17 June. This year a record number of 22 teams
                  participated and they submitted 234 solutions. Most solutions
                  are
                  significantly better than the solutions in 2018. This
                  challenge has
                  two different tracks: object detection and image
                  classification.</p>
                <p>In the object detection track, the first prize is obtained by
                  the
                  Amazon team; the members are Tao Sheng, Peng Lei, Zhuo Deng,
                  Lu Xia,
                  Yuyin Sun, Feng Ye, Lijun Li, Yang Liu, Cheng Hao Kuo, Ning
                  Zhou, Jim
                  Thomas, Kah Kuen Fu, and Dan Chernikoff. The second prize goes
                  to the
                  Tsinghua University China team composed of Wenshuo Li, Chengyu
                  Shi,
                  Zhihang Li, and Teng Xi. The third prize is received by the
                  Qualcomm
                  team, including Jiancheng Lyu, Shuai Zhang, Mengran Gou,
                  Yingyong Qi,
                  and Ning Bi.</p>
                <p>In the image classification track, the first Prize goes to
                  Deepinali
                  from Alibaba. The team members are Chen Hesen, Wang Zhibin,
                  Dou
                  Zesheng, Sun Xiuyu, and Li Hao. Expasoft received the second
                  prize and
                  the team includes Sergey Alyamkin, Alexandr Goncharenko, David
                  Svitov,
                  Andrey Denisov, and Timofey Naumenko. The third prize is the
                  MIT HAN
                  Lab from Massachusetts Institute of Technology. The members
                  are Han
                  Cai, Tianzhe Wang, Zhanghao Wu, Kuan Wang, Ji Lin, Song
                  Han.</p>
                <p>LPIRC started in 2015 with the aim to identify the
                  technologies for
                  computer vision using energy efficiently. IEEE Rebooting
                  Computing has
                  been the primary sponsor and provides administrative support.
                  In 2019,
                  sponsors are Google, Mediatek, Xilinx, and IEEE Circuits and
                  Systems
                  Society. Past sponsors include Facebook, Nvidia, IEEE
                  GreenICT, IEEE
                  Council of Design Automation, IEEE Council on
                  Superconductivity. The
                  2019 LPIRC is organized by Purdue University, Duke University,
                  University of North Carolina Chapel Hill. Google has been
                  providing a
                  unified latency metric and an evaluation platform for the
                  online track
                  (also called On-Device Visual Intelligence Challenge) since
                  2018.</p>
                <p style="font-size:larger;text-align:center;"><strong>Thanks to
                    all
                    participants, organizing committee members, and student
                    assistants!</strong></p>
                <p>&nbsp;</p>
                <h2 style="border-bottom: 1px dotted;">LPIRC 2019 Gallery</h2>
                <div style="text-align: center;"><a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/image-from-ios-2.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/image-from-ios-2.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/image-from-ios-4.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/image-from-ios-4.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_100408_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_100408_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_101016_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_101016_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_110705_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_110705_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_120408_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_120408_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_120419_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_120419_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_133111_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_133111_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_140218_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_140218_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_185433_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_185433_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_203029_p.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/P_20190617_203029_p.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2995.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2995.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2996.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2996.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2998.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2998.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2999.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_2999.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_6313.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_6313.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9494.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9494.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9524.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9524.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9532.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9532.jpg"
                      alt="LPIRC photo" width="250" height="188"></a> <a
                    href="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9536.jpg"><img
                      style="margin-bottom: 15px; margin-right: 15px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/lpirc/2019/IMG_9536.jpg"
                      alt="LPIRC photo" width="250" height="188"></a></div>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">LPIRC 2019 Overview</h2>
                <p>This workshop will extend the successes of LPIRC in the past
                  four
                  years identifying the best vision solutions that can
                  simultaneously
                  achieve high accuracy in computer vision and energy
                  efficiency. Since
                  the first competition held in 2015, the winners’ solutions
                  have
                  improved 24 times in the ratio of accuracy divided by
                  energy.</p>
                <p>LPIRC 2019 will continue the online + onsite options for
                  participants. One online track with pre-selected hardware
                  allows
                  participants to submit their solutions multiple times without
                  the need
                  of traveling. One onsite track allows participants to bring
                  their
                  systems to CVPR.</p>
                <ul>
                  <li>Online Track (objection detection): Tensorflow Model.
                    Submission
                    dates: 2019/05/15-2019/06/10</li>
                  <li>Onsite Track (objection detection): No hardware or
                    software
                    restriction. Bring your system to CVPR.</li>
                </ul>
                <p>More details of the tracks are described below.</p>
                <p>For the onsite track, Xilinx Ultra96 + PYNQ framework <a
                    href="http://www.pynq.io/community.html">http://www.pynq.io/community.html</a>
                  is an option available to participants. Xilinx will provide
                  support
                  and possible HW donation. People interested in this
                  opportunity may
                  contact Naveen Purushotham (<a
                    href="mailto:npurusho@xilinx.com">npurusho@xilinx.com</a>)
                  for more
                  details.</p>
                <p>All participants must register before 2019/05/30.
                  Registration open
                  2019/03/15 at <a
                    href="https://lpirc.ecn.purdue.edu/">https://lpirc.ecn.purdue.edu/</a>.</p>
                <p>To win a prize in 2019 LPIRC, a solution must be better than
                  the
                  state of the art. Please note that this is a new requirement
                  in 2019.
                  The requirement in each track is shown below. If a solution’s
                  score is
                  below the threshold performance, it cannot win. If a
                  solution’s score
                  is better, it still has to beat the other solutions submitted
                  to 2019
                  LPIRC.</p>
                <p>Online Track: mAP 0.14684<br>
                  Onsite Track: mAP / energy (WH) 0.4446</p>
                <p>LPIRC 2019 is a one-day workshop and will feature three
                  speakers, a
                  session on future low-power computer vision, and the onsite
                  competition. The program for LPIRC is shown below.</p>
                <p>2019 winners will be invited to speak in a workshop
                  co-located with
                  2019 ICCV.</p>
                <div class="mobile-table">
                  <table class="table table-hover table-striped">
                    <tbody>
                      <tr>
                        <td width="10%">09:30-09:40</td>
                        <td width="90%"><a
                            href="http://ieeetv.ieee.org/conference-highlights/ieee-welcome-lpirc-2018?rf=series|3">Welcome
                            by Organizers and Summary of Online
                            Challenge</a></td>
                      </tr>
                      <tr>
                        <td>09:40-10:30</td>
                        <td>Speech by the Competition Winners (2018
                          LPIRC-II)<br>
                          <ul>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/award-winning-methods-for-lpirc-tao-sheng-lpirc-2018?rf=series|3">Award-Winning
                                Methods for LPIRC</a> - Amazon: Tao Sheng (<a
                                href="mailto:tsheng@amazon.com">tsheng@amazon.com</a>)</li>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/approach-to-winning-solutions-alexander-goncharenko-lpirc-2018?rf=series|3">Approach
                                to Winning Solutions</a> - Expasoft: Alexander
                              Goncharenko (<a
                                href="mailto:a.goncharenko@expasoft.ru">a.goncharenko@expasoft.ru</a>)
                              and Sergey Alyamkin (<a
                                href="mailto:s.alyamkin@expasoft.ru">s.alyamkin@expasoft.ru</a>)</li>
                            <ul>
                              <li><a
                                  href="https://rebootingcomputing.ieee.org/images/files/pdf/winning-solution-on-lpirc-ii-competition.pdf"
                                  target="_blank">Winning Solution on LPIRC-ll
                                  Competition</a> (PDF, 1 MB)</li>
                            </ul>
                          </ul>
                        </td>
                      </tr>
                      <tr>
                        <td>10:30-11:10</td>
                        <td>Invited Speaker: <a
                            href="http://ieeetv.ieee.org/conference-highlights/deeper-neural-networks-kurt-keutzer-lpirc-2018?rf=series|3">Rethinking
                            the Computations in Computer Vision (and the
                            Hardware that
                            Computes Them)</a>, Kurt Keutzer, Berkeley</td>
                      </tr>
                      <tr>
                        <td>11:10-12:00</td>
                        <td>Invited Speakers<br>
                          <ul>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/deep-learning-machine-learning-inference-ashish-sirasao-lpirc-2018?rf=series|3">Turbocharge
                                Deep Learning Inference using Reconfigurable
                                Platforms</a>, Ashish Sirasao (<a
                                href="mailto:asirasa@xilinx.com">asirasa@xilinx.com</a>),
                              Xilinx</li>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/the-art-of-mobilenet-design-andrew-howard-lpirc-2018?rf=series|3">The
                                Art of MobileNet Design</a>, Andrew Howard (<a
                                href="mailto:howarda@google.com">howarda@google.com</a>),
                              Google</li>
                          </ul>
                        </td>
                      </tr>
                      <tr>
                        <td>12:00-12:40</td>
                        <td>Lunch Break</td>
                      </tr>
                      <tr>
                        <td>12:40-14:40</td>
                        <td>Invited Speakers<br>
                          <ul>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/co-design-of-algorithms-hardware-for-dnns-vivienne-sze-lpirc-2018?rf=series|3">Understanding
                                the Challenges of Algorithm and Hardware
                                Co-design for
                                Deep Neural Networks</a>, Vivienne Sze (<a
                                href="mailto:sze@mit.edu">sze@mit.edu</a>),
                              MIT</li>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/quantization-without-fine-tuning-tijimen-blankevoort-lpirc-2018?rf=series|3">Efficient
                                Deep Learning: Quantizing models without
                                re-training</a>, Tijmen Blankevoort (<a
                                href="mailto:tijmen@qti.qualcomm.com">tijmen@qti.qualcomm.com</a>),
                              Qualcomm</li>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/visual-wake-words-challenge-aakanksha-chowdhery-lpirc-2018?rf=series|3">Visual
                                Wake Words Challenge</a>, Aakanksha Chowdhery
                              (<a
                                href="mailto:chowdhery@google.com">chowdhery@google.com</a>),
                              and Pete Warden (<a
                                href="mailto:petewarden@google.com">petewarden@google.com</a>),
                              Google</li>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/classifying-attention-in-pivotal-response-treatment-videos-corey-heath-lpirc-2018?rf=series|3">Are
                                You Paying Attention? Classifying Attention in
                                Pivotal
                                Response Treatment Videos</a>, Corey D C Heath
                              (<a
                                href="mailto:corey.heath@asu.edu">corey.heath@asu.edu</a>),
                              Hemanth Venkateswara (<a
                                href="https://rebootingcomputing.ieee.org/hemanthv@asu.edu">hemanthv@asu.edu</a>),
                              Sethuraman Panchanathan, (<a
                                href="mailto:panch@asu.edu">panch@asu.edu</a>)</li>
                            <ul>
                              <li><a
                                  href="https://rebootingcomputing.ieee.org/images/files/pdf/are-you-paying-attention-classifying-attention-in-pivotal-response-treatment-videos.pdf"
                                  target="_blank">Are You Paying Attention?
                                  Classifying
                                  Attention in Pivotal Response Treatment
                                  Videos</a>
                                (PDF, 2 MB)</li>
                            </ul>
                            <li><a
                                href="http://ieeetv.ieee.org/conference-highlights/designing-efficient-on-device-ai-aakanksha-chowdhery-lpirc-2018?rf=series|3">Designing
                                efficient on-device AI using Tensorflow
                                Lite</a>,
                              Aakanksha Chowdhery (<a
                                href="mailto:chowdhery@google.com">chowdhery@google.com</a>)
                              and Bo Chen (<a
                                href="mailto:bochen@google.com">bochen@google.com</a>),
                              Google</li>
                          </ul>
                        </td>
                      </tr>
                      <tr>
                        <td>14:40-17:00</td>
                        <td>Onsite Competition</td>
                      </tr>
                      <tr>
                        <td>17:00-17:30</td>
                        <td>Discussion: Future Low-Power Computer Vision</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p>If you are interested in discussion of future competitions of
                  low-power computer vision, please fill out <a
                    href="https://docs.google.com/forms/d/e/1FAIpQLScI0SRwMmn5caSAbByqZcd1yjBmNuufaGPrgpGS6HGMcYm81g/viewform?usp=sf_link">this
                    form</a>.</p>
                <p>For inquiries, please contact <a
                    href="mailto:rcinfo@ieee.org">rcinfo@ieee.org</a>.</p>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">LPIRC 2019 Links</h2>
                <!-- <p><a href="https://rebootingcomputing.ieee.org/lpirc/grand-challenge-in-low-power-computer-vision">Call for Papers:  Grand Challenge for Low-Power Computer Vision</a></p> -->
                <p><a
                    href="https://rebootingcomputing.ieee.org/lpirc/onsite-track">LPIRC
                    Onsite Track</a></p>
                <p><a
                    href="https://rebootingcomputing.ieee.org/lpirc/online-track">LPIRC
                    Online Track</a></p>
                <p><a
                    href="https://rebootingcomputing.ieee.org/lpirc/invited-talks">Abstracts
                    of Invited Talks</a></p>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Sponsors</h2>
                <p><a href="https://rebootingcomputing.ieee.org"><img
                      src="https://rebootingcomputing.ieee.org/images/files/images/ieee-rebooting-computing.png"
                      alt="IEEE Rebooting Computing" width="110" height="70"
                      style="margin:15px 20px 0px 0px;vertical-align:middle;"></a><a
                    href="https://www.google.com/"><img
                      src="https://rebootingcomputing.ieee.org/images/files/images/google.jpg"
                      alt="Google" width="130" height="43"
                      style="margin:15px 20px 0px 0px;vertical-align:middle;"></a><a
                    href="http://ieee-cas.org/"><img
                      src="https://rebootingcomputing.ieee.org/images/files/images/ieee-circuits-and-systems-society.jpg"
                      alt="IEEE Circuits and Systems Society" width="90"
                      height="98"
                      style="margin:15px 20px 0px 0px;vertical-align:middle;"></a><a
                    href="https://www.xilinx.com/"><img
                      src="https://rebootingcomputing.ieee.org/images/files/images/xilinx.jpg"
                      alt="Xilinx" width="130" height="27"
                      style="margin:15px 20px 0px 0px;vertical-align:middle;"></a><a
                    href="https://www.mediatek.com/"><img
                      style="margin: 20px 20px 0px 0px; vertical-align: middle;"
                      src="https://rebootingcomputing.ieee.org/images/files/images/mediatek.png"
                      alt="MediaTek" width="158" height="40"></a></p>
                <p>&nbsp;</p>
                <p>LPIRC 2019 is sponsored by Xilinx, Google, Mediatek, IEEE
                  Circuits
                  and Systems Society.</p>
                <p>LPIRC 2015-2018 sponsors include Google, Facebook, Nvidia,
                  Xilinx,
                  Mediatek, IEEE Circuits and Systems Society, IEEE Council on
                  Electronic Design Automation, IEEE GeenICT, and IEEE Council
                  on
                  Superconductivity. IEEE Rebooting Computing is the founding
                  sponsor of
                  LPIRC.</p>
                <p><a
                    href="https://rebootingcomputing.ieee.org/lpirc/sponsorship">Sponsorship
                    Opportunities Available</a></p>
                <p>&nbsp;</p>
                <h2 style="border-bottom:1px dotted;">Organizing Committee</h2>
                <ul>
                  <li>Yung-Hsiang Lu (Purdue)</li>
                  <li>Yiran Chen (Duke)</li>
                  <li>Alex Berg (UNC)</li>
                  <li>Bo Chen (Google)</li>
                </ul>

                <div class="clearfix"></div></div></article>
          </div>
        </div>

      </div>

    </main>

    <!-- Footer Section -->
    <footer id="footer" class="footer bg-dark text-light py-3">
      <div class="container text-center">
        <p class="mb-0">© Copyright <strong>LPVC</strong>. All Rights
          Reserved</p>
        <p class="mb-0">Designed by <a href="https://bootstrapmade.com/"
            class="text-success">BootstrapMade</a> Maintained by <span
            class="text-success">Ron Natarajan and Basil Khwaja</span></p>
      </div>
    </footer>

    <!-- Scroll Top -->
    <a href="#" id="scroll-top"
      class="scroll-top d-flex align-items-center justify-content-center"><i
        class="bi bi-arrow-up-short"></i></a>

    <!-- Preloader -->
    <div id="preloader"></div>

    <!-- Vendor JS Files -->
    <script
      src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="../assets/vendor/php-email-form/validate.js"></script>
    <script src="../assets/vendor/aos/aos.js"></script>
    <script
      src="../assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="../assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script
      src="../assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
    <script
      src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="../assets/vendor/swiper/swiper-bundle.min.js"></script>

    <!-- Main JS File -->
    <script src="../assets/js/main.js"></script>

  </body>

</html>
