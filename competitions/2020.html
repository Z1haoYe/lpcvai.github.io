{% extends 'competitions/base.html' %}
{% load static %}
{% block cardbody %}
<h1 style="text-align:center; border-bottom:1px dotted;">2020 IEEE Low-Power Computer Vision Challenge</h1>
<!--Begin Content-->
<div class="card" style="height:fit-content">
    <div class="card-body"style="height:fit-content">
        <!--The 2020 LPCVC has three tracks: 
        <ul>
        <li>Recognizing English letters in video taken by an unmanned aerial vehicle (also called drone) using PyTorch Mobile</li>
        <li>Recognizing objects in images using Field Programming Gate Array (FPGA)</li>
        <li>Recognizing objects in images using TensorFlow, also called On-Device Visual Intelligence Challenge (OVIC)</li>
        </ul>-->
        <h2>LPCVC 2020 Overview</h2>
        <p>The 2020 CVPR Workshop of Low-Power Computer Vision Challenge was succesfully hosted on <strong>15 June 2020 in Seattle, USA</strong>.
          This workshop featured the winners of the past Low-Power Image Recognition Challenges, invited speakers from academia and industry, as well as presentations of accepted papers.</p>

          <p style="text-align:center;"><a href="http://cvpr2020.thecvf.com/"><img src="https://rebootingcomputing.ieee.org/images/files/images/cvpr-2020.jpg" style="max-width:100%" alt="Seattle, Washington, USA" width="450" height="300"></a></p>
          <p style="text-align:center;"><a href="http://cvpr2020.thecvf.com/">cvpr2020.thecvf.com</a></p>

          <p>The LPCVC featured 3 tracks sponsored by Facebook, Xilinx, and Google was be hosted online from <b>7/1/2020 to 7/31/2020</b>. 
          More details are posted below. Please join the online discussion forum 
          <a href="https://lpcvc.slack.com/join/shared_invite/enQtOTQzMzA5OTg0NzA0LTY4YjY2YmI2NmM0YTBhMTk1NmEwZjI4NTAxYTEyZTkyZmE4ZWVhZWQ0Y2NkOGFhZmY4NzI0YjU4OGUzMWUwNjE">lpcvc.slack.com</a>.</p>
          
          <p>The 2020 IEEE Low-Power Computer Vision Challenge (LPCVC) concluded successfully on 2020/07/31, after a month of competition. Forty six teams submitted 378 solutions in four tracks. 
          During the month-long challenge, many teams submitted multiple solutions and witnessed the scores becoming much higher. 
          The 2020 LPCVC adds a video track and expands the Low-Power Image Recognition Challenge (LPIRC) started in 2015.</p>
        <!-- <p>The participant agreement for the LPCVC 2020 contest can be found <a href="https://drive.google.com/file/d/1RbpVmdFSxKne9pj28J_nKOi9P6HO6Y_K/view">here</a>.</p> -->
        <hr>
        <h2>Tracks</h2>
        <div class="container">
          <div class="row">
            <div class="col">
              <div class="row">
                <h4 class="title">Online Track - PyTorch UAV Video</h4>
                <p class="description mb-2">
                  Optical character recognition of characters in video captured by an unmanned aerial vehicle (UAV).
                </p>
                <a class="align-items-end " href="{% url '2020CVPR/video-info' %}" target="_blank">More</a>
              </div>
            </div>
            <div class="col border-left border-right">
              <h4 class="title">Online Track - FPGA Image</h4>
              <p class="description mb-2">
                Accurate and low-power classification of objects from 1000 ImageNet categories.
              </p>
              <a class="align-items-end " href="{% url '2020CVPR/image-info' %}" target="_blank">More</a>
            </div>
            <div class="col">
              <h4 class="title">Online Track - OVIC Image</h4>
              <p class="description mb-2">
                A competition for real-time detection and classification.
              </p>
              <a class="align-items-end " href="{% url '2020CVPR/ovic-info' %}" target="_blank">More</a>
            </div>
          </div>
      
        </div>

        <p style="margin:0px">&nbsp;</p>
        For more information, go to the <a href="https://lpcv.ai/2020CVPR/introduction#">competition</a> introduction page. To see a detailed score breakdown for each submission, go to the <a href="https://lpcv.ai/scoreboard/Video20">leaderboard.</a>
        <p style="margin:0px">&nbsp;</p>

        <!--==========================
          Drone Video Track Section
        ============================-->
        <a style="color:#666666" href="https://lpcv.ai/2020CVPR/video-track"><h3>PyTorch UAV Video Track</h3></a>
        <p>This track requires that participating teams recognize English letters or numbers in the signs in the video 
          captured by an unmanned aerial vehicle (UAV). The following two video frames show examples of such a challenge. 
          The challenge appears in the form of question-answer pairs. A successful solution has to answer the English 
          letters or numbers appearing in the same frame as the questions. For example, if the question is “RESTRICTED AREA”, 
          the answer should be “B056 ELEVATOR CONTROL ELEVATOR PERSONNEL ONLY”. If the question is “CONFERENCE”, 
          the answer should be “122”. The solutions are case insensitive. The solutions must use PyTorch running on Raspberry Pi 3B+. 
        </p>
        <img src="{% static 'img/conf.png' %}" style="max-width: 50%; max-height: 50%;"> <img src="{% static 'img/resArea.png' %}" style="max-width: 50%; max-height: 50%;">
        <br><br>
        <p>Eleven teams submitted 84 solutions to this track. The winners of this tracks are:</p>
        <table class="table table-bordered">
            <thead>
              <th>Prize</th>
              <th>Team</th>
              <th>Organization</th>
              <th>Member</th>
              <th>Representative</th>
            </thead>
            <tbody>
              <tr>
                <td>First</td>
                <td>LPNet</td>
                <td>ByteDance Inc</td>
                <td><ol>
                  <li>Xuefeng Xiao</li>
                  <li>Xing Wang</li>
                  <li>Li Lin</li>
                  <li>Tianyu Zhao</li>
                  <li>Ni Zhuang</li>
                  <li>Huixia Li</li>
                  <li>Xin Xia</li>
                  <li>Can Huang</li>
                  <li>Linfu Wen</li>
                </ol></td>
                <td>Xuefeng Xiao (<a href="xiaoxuefeng.ailab@bytedance.com">xiaoxuefeng.ailab@bytedance.com</a>)</td>
              </tr>
              <tr>
                <td>Second</td>
                <td>TAMU-KWAI</td>
                <td>Texas A & M University and Kwai Inc</td>
                <td><ol>
                  <li>Yunhe Xue*</li>
                  <li>Zhenyu Hu*</li>
                  <li>Rahul Sridhar*</li>
                  <li>Zhenyu Wu*</li>
                  <li>Pengcheng Pi</li>
                  <li>Jiayi Shen</li>
                  <li>Jianchao Tan</li>
                  <li>Xiangru Lian</li>
                  <li>Zhangyang Wang</li>
                  <li>Ji Liu</li>
                </ol></td>
                <td>Zhenyu Wu (<a href="wuzhenyu_sjtu@tamu.edu">wuzhenyu_sjtu@tamu.edu</a>)</td>
              </tr>
              <tr>
                <td>Third</td>
                <td>C408</td>
                <td>Stony Brook University (SUNY Korea)</td>
                <td><ol>
                  <li>Seonghwan Jeong</li>
                  <li>Jay Hoon Jung</li>
                  <li>YoungMin Kwon</li>
                </ol></td>
                <td>Seonghwan Jeong (<a href="seonghwan.jeong@stonybrook.edu">seonghwan.jeong@stonybrook.edu</a>)</td>
              </tr>
            </tbody>
          </table>
        <p style="margin:0px">&nbsp;</p>

        <!--==========================
          FPGA Image Track Section
        ============================-->
        <a style="color:#666666" href="https://lpcv.ai/2020CVPR/image-track"><h3>FPGA Image Track</h3></a>
        <p>This track uses Ultra96-V2 + Xilinx® Deep Learning Processor Unit (DPU) running software PYNQ + Vitis AI.
           The challenge recognizes objects in images. Nineteen teams submitted 115 solutions. The winners of this track are:</p>
        <table class="table table-bordered">
            <thead>
              <th>Prize</th>
              <th>Team</th>
              <th>Organization</th>
              <th>Member</th>
              <th>Representative</th>
            </thead>
            <tbody>
              <tr>
                <td>First</td>
                <td>OnceForAll</td>
                <td>MIT HAN Lab</td>
                <td><ol>
                  <li>Zhekai Zhang*</li>
                  <li>Han Cai*</li>
                  <li>Song Han</li>
                  <li>Yubei Chen</li>
                  <li>Jeffrey Pan</li>
                  <li>Ligeng Zhu</li>
                  <li>Tianzhe Wang</li>
                  <li>Song Han</li>
                </ol></td>
                <td>Zhekai Zhang (<a href="(zhangzk@mit.edu)">(zhangzk@mit.edu)</a>)</td>
              </tr>
              <tr>
                <td>Second</td>
                <td>MAXX</td>
                <td>National Chiao Tung University, Taiwan</td>
                <td><ol>
                  <li>Shih-Yu Wei</li>
                  <li>Xuan-Hong Li</li>
                  <li>Yu-Da Ju</li>
                  <li>Juinn-Dar Huang</li>
                </ol></td>
                <td>Xuan-Hong Li (<a href="nonelee.ee05@g2.nctu.edu.tw">nonelee.ee05@g2.nctu.edu.tw</a>)</td>
              </tr>
              <tr>
                <td>Third</td>
                <td>Water</td>
                <td>SKLCA, Institute of Computing Technology, CAS</td>
                <td><ol>
                  <li>Shengwen Liang</li>
                  <li>Rick Lee</li>
                  <li>Ying Wang</li>
                  <li>Cheng Liu</li>
                  <li>Huawei Li</li>
                </ol></td>
                <td>Shengwen Liang (<a href="liangshengwen@ict.ac.cn">liangshengwen@ict.ac.cn</a>)</td>
              </tr>
            </tbody>
          </table>
          <p style="margin:0px">&nbsp;</p>

          <!--==========================
          OVIC Section
          ============================-->
          <a style="color:#666666" href="https://lpcv.ai/2020CVPR/ovic-track"><h3>OVIC Tensorflow Track</h3></a>
          The OVIC Tensorflow track has three different challenges: 
        <ul>
        <li><b>Interactive object detection:</b> This category focuses on COCO detection models operating at 30 ms / image on a Pixel 4 smartphone (CPU).</li>
        <li><b>Real-time image classification on Pixel 4: </b>This category focuses on Imagenet classification models operating at 10 ms / image on a Pixel 4 smartphone (CPU).</li>
        <li><b>Real-time image classification on LG G8:</b> This category focuses on Imagenet classification models operating at 7 ms / image on a LG G8 smartphone (DSP).</li>
        </ul>
        <p>Sixteen teams submitted 179 solutions to these tracks. The winners of these tracks are:</p>
        <h4>Interactive Object Detection</h4>
        <table class="table table-bordered">
            <thead>
              <th>Prize</th>
              <th>Team</th>
              <th>Organization</th>
              <th>Member</th>
              <th>Representative</th>
            </thead>
            <tbody>
              <tr>
                <td>First</td>
                <td>OnceForAll</td>
                <td>MIT HAN Lab</td>
                <td><ol>
                  <li>Yubei Chen*</li>
                  <li>Jeffrey Pan*</li>
                  <li>Han Cai</li>
                  <li>Zhekai Zhang</li>
                  <li>Tianzhe Wang</li>
                  <li>Ligeng Zhu</li>
                  <li>Song Han</li>
                </ol></td>
                <td>Yubei Chen (<a href="yubeic@berkeley.edu">yubeic@berkeley.edu</a>)</td>
              </tr>
              <tr>
                <td>Second</td>
                <td>SjtuBicasl</td>
                <td>Shanghai Jiao Tong University</td>
                <td><ol>
                  <li>Lining Hu</li>
                  <li>Xinzi Xu</li>
                  <li>Yongfu Li</li>
                  <li>Weihong Yan</li>
                  <li>Xiaocui Li</li>
                  <li>Yuxin Ji</li>
                </ol></td>
                <td>Xinzi Xu (<a href="xu_xinzi@sjtu.edu.cn">xu_xinzi@sjtu.edu.cn</a>)</td>
              </tr>
              <tr>
                <td>Third</td>
                <td>Novauto</td>
                <td>Novauto Inc</td>
                <td><ol>
                  <li>Changcheng Tang</li>
                  <li>Tianyi Lu</li>
                  <li>Zhiyong Zhang</li>
                  <li>Shuang Liang</li>
                  <li>Tianchen Zhao</li>
                  <li>Xuefei Ning</li>
                  <li>Shiyao Li</li>
                  <li>Hanbo Sun</li>
                  <li>Shulin Zeng</li>
                </ol></td>
                <td>Changcheng Tang (<a href="angcc1127@gmail.com">angcc1127@gmail.com</a>)</td>
              </tr>
            </tbody>
          </table>
          <p style="margin:0px">&nbsp;</p>
          <h4>Real-time Image Classification on Pixel 4</h4>
          <table class="table table-bordered">
            <thead>
              <th>Prize</th>
              <th>Team</th>
              <th>Organization</th>
              <th>Member</th>
              <th>Representative</th>
            </thead>
            <tbody>
              <tr>
                <td>First</td>
                <td>BAIDU&THU</td>
                <td>Baidu Inc & Tsinghua University</td>
                <td><ol>
                  <li>Zerun Wang</li>
                  <li>Teng Xi</li>
                  <li>Dahan Gong</li>
                  <li>Fei Tian</li>
                  <li>Zizhou Jia</li>
                  <li>Cheng Cui</li>
                  <li>Xiaoyu Huang</li>
                  <li>Zhihang Li</li>
                  <li>Fan Yang</li>
                  <li>Fukui Yang</li>
                  <li>Tianxiang Hao</li>
                  <li>Shengzhao Wen</li>
                  <li>Huiyue Yang</li>
                  <li>Gang Zhang</li>
                  <li>Guiguang Ding</li>
                </ol></td>
                <td>Zerun Wang (<a href="wangzrthu@gmail.com">wangzrthu@gmail.com</a>)</td>
              </tr>
              <tr>
                <td>Second</td>
                <td>imchinfei</td>
                <td>Beihang University</td>
                <td><ol>
                  <li>Fei Qin</li>
                  <li>Yangyang Kuang</li>
                </ol></td>
                <td>Fei Qin (<a href="64242200@qq.com">64242200@qq.com</a>)</td>
              </tr>
              <tr>
                <td>Third</td>
                <td>LPNet</td>
                <td>ByteDance Inc</td>
                <td><ol>
                  <li>Xuefeng Xiao</li>
                  <li>Jiashi Li</li>
                  <li>Xin Xia</li>
                  <li>Huixia Li</li>
                  <li>Linfu Wen</li>
                </ol></td>
                <td>Xuefeng Xiao (<a href="xiaoxuefeng.ailab@bytedance.com">xiaoxuefeng.ailab@bytedance.com</a>)</td>
              </tr>
            </tbody>
          </table>
          <p style="margin:0px">&nbsp;</p>
          <h4>Real-time Image Classification on LG G8</h4>
          <table class="table table-bordered">
            <thead>
              <th>Prize</th>
              <th>Team</th>
              <th>Organization</th>
              <th>Member</th>
              <th>Representative</th>
            </thead>
            <tbody>
              <tr>
                <td>First</td>
                <td>LPNet</td>
                <td>ByteDance Inc</td>
                <td><ol>
                    <li>Xuefeng Xiao</li>
                    <li>Jiashi Li</li>
                    <li>Xin Xia</li>
                    <li>Huixia Li</li>
                    <li>Linfu Wen</li>
                </ol></td>
                <td>Xuefeng Xiao (<a href="xiaoxuefeng.ailab@bytedance.com">xiaoxuefeng.ailab@bytedance.com</a>)</td>
              </tr>
              <tr>
                <td>Second</td>
                <td>imchinfei</td>
                <td>Beihang University</td>
                <td><ol>
                  <li>Fei Qin</li>
                  <li>Yangyang Kuang</li>
                </ol></td>
                <td>Fei Qin (<a href="64242200@qq.com">64242200@qq.com</a>)</td>
              </tr>
              <tr>
                <td>Third (Tie)</td>
                <td>OnceForAll</td>
                <td>MIT HAN Lab</td>
                <td><ol>
                    <li>Yubei Chen*</li>
                    <li>Jeffrey Pan*</li>
                    <li>Han Cai</li>
                    <li>Zhekai Zhang</li>
                    <li>Tianzhe Wang</li>
                    <li>Ligeng Zhu</li>
                    <li>Song Han</li>
                </ol></td>
                <td>Yubei Chen (<a href="yubeic@berkeley.ed">yubeic@berkeley.ed</a>)</td>
              </tr>
              <tr>
                <td>Third (Tie)</td>
                <td>FoxPanda</td>
                <td>National Chiao Tung University and National Tsing Hua University.</td>
                <td><ol>
                    <li>Chia-Hsiang Liu</li>
                    <li>Wei-Xiang Guo</li>
                    <li>Yuan-Yao Sung</li>
                    <li>Yi Lee</li>
                    <li>Kai-Chiang Wu</li>
                </ol></td>
                <td>jacoblau.cs08g@nctu.edu.tw (<a href="jacoblau.cs08g@nctu.edu.tw">jacoblau.cs08g@nctu.edu.tw</a>)</td>
              </tr>
            </tbody>
          </table>
          <p>LPCVC started in 2015 with the aim to identify the technologies for computer vision using energy efficiently. 
          IEEE Rebooting Computing has been the primary financial sponsor and provides administrative support. A new IEEE technical 
          committee has been created to coordinate future activities of low-power computer vision. People interested in 
          joining the committee please contact Terence Martinez, Program Director Future Directions IEEE Technical 
          Activities, t.c.martinez@ieee.org.</p>
          

          <!--==========================
          Sponsors Section
          ============================-->
          <h3>Sponsors</h3>
          <center><div class="sponsors-images">
            <a href="https://pytorch.org/"><img class="img-responsive-80 img-fluid sponsors-logo" src="{% static 'img/sponsors/PyTorch.png' %}" alt="PyTorch"></a>
            <a href="https://www.xilinx.com/"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/xilinx_logo.svg' %}" alt="Xilinx"></a>
            <a href="https://about.google/"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/google_logo.jpeg' %}" alt="Google"></a>
          </div>
          <h2 style="font-size: 1px"></h2>
          <div class="sponsors-images">
            <a href="https://ieee-cas.org/"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/cas_logo.png' %}" alt="IEEE CAS"></a>
            <a href="https://ieee-ceda.org/"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/ceda_logo.png' %}" alt="IEEE CEDA"></a>
            <a href="https://rebootingcomputing.ieee.org"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/ieee-rebooting-computing.png' %}" alt="Rebooting Computing" style=""></a>
            <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1925713"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/nsf_logo.jpeg' %}" alt="NSF"></a>
            <a href="https://www.mediatek.com/"><img class="img-responsive-80 sponsors-logo"src="{% static 'img/sponsors/mediatek.png' %}" alt="Mediatek" style=""></a>
            <a href="http://www.emc.com.tw/emc/tw/"><img class="img-responsive-80 sponsors-logo" src="{% static 'img/sponsors/elanlogo.jpeg' %}" alt="Elan Microelectronics" style=""></a>
          </div></center>
          <!--==========================
          Program Recrodings
          ===========================-->
          <h3>Program Recordings</h3>
          <table class="table table-hover table-striped"><tbody>
            <tr><th width="10%;">Time</th><th width="90%;">Speaker</th></tr>
            <tr><td>06:00</td>
              <td>Organizers and Sponsors’ Welcome
                <br>
                <ol>
                  <li>Terence Martinez, IEEE</li>
              <li>Yung-Hsiang Lu, Purdue</li>
              <li>Yiran Chen, Duke</li>
              <li>Ming-Ching Chang, SUNY Albany</li>
              <li>Joe Spisak, Facebook</li>
              <li>Bo Chen, Google </li>
              <li>Ashish	Sirasao, Xilinx</li>
              <li>Yao-Wen Chang, IEEE CEDA </li>
              <li>Yen-Kuang Chen, IEEE CASS </li>
              <li>ELan Microelectronics</li></ol></td>
            </tr><tr><td>06:05</td><td>History of Low-Power Computer Vision Challenge, Terence Martinez, IEEE </td></tr>
            <tr><td>06:10</td><td>Paper Presentations:
            <ol><li><div class="row"><div class="col-8">Challenges in Energy-Efficient Deep Neural Network Training with FPGA], Yudong Tao (University of Miami)*; Ma Rui (University of Miami); Mei-Ling Shyu (University of Miami); Shu-Ching Chen (Florida International University)
                  </div><div class="col-4"><img style="margin-top:-30px; width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/10/10-teaser.gif" alt=""></div></div></li><li><div class="row"><div class="col-8">CSPNet: A New Backbone that can Enhance Learning Capability of CNN], Chien-Yao Wang (Institute of Information Science, Academia Sinica)*; Hong-Yuan Mark Liao (Institute of Information Science, Academia Sinica, Taiwan); Yueh-Hua Wu (National Taiwan University / Academia Sinica); Ping-Yang Chen (Department of Computer Science, National Chiao Tung University); Jun-Wei Hsieh (College of Artificial Intelligence and Green Energy ); I-Hau Yeh (Elan Microelectronics Corporation)
                  </div><div class="col-4"><img class="mt-2" style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/2/2-teaser.gif" alt=""></div></div></li><li class="mt-4"><div class="row"><div class="col-8">Recursive Hybrid Fusion Pyramid Network for Real-Time Small Object Detection on Embedded Devices], Ping-Yang Chen (Department of Computer Science, National Chiao Tung University); Jun-Wei Hsieh (College of Artificial Intelligence and Green Energy )*; Chien-Yao Wang (Institute of Information Science, Academia Sinica); Hong-Yuan Mark Liao (Institute of Information Science, Academia Sinica, Taiwan)
                  </div><div class="col-4"><img style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/11/11-teaser.gif" alt=""></div></div></li><li class="mt-4"><div class="row"><div class="col-8">Enabling Incremental Knowledge Transfer for Object Detection at the Edge], Mohammad Farhadi Bajestani (Arizona state university)*; Mehdi Ghasemi (Arizona state university); Sarma Vrudhula (Arizona State university); Yezhou Yang (Arizona State University)
                  </div><div class="col-4"><img style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/7/7-teaser.gif" alt=""></div></div></li><li><div class="row"><div class="col-8">Effective Deep-Learning-Based Depth Data Analysis on Low-Power Hardware for Supporting Elderly Care], Christopher Pramerdorfer (Computer Vision Lab - TU Wien)*; Martin Kampel (Vienna University of Technology, Computer Vision Lab); Rainer Planinc (CogVis)
                  </div><div class="col-4"><img style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/6/6-teaser.gif" alt=""></div></div></li></ol></td></tr><tr><td>07:00</td><td>
            Poster Presentations:
            <ol><li><div class="row"><div class="col-8">Enabling monocular depth perception at the very edge], Valentino Peluso (Politecnico di Torino); Antonio Cipolletta (Politecnico di Torino); Andrea Calimera (Politecnico di Torino)*; Matteo Poggi (University of Bologna); Fabio Tosi (University of Bologna); Filippo Aleotti (University of Bologna); Stefano Mattoccia (University of Bologna)
                  </div><div class="col-4"><img style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/3/3-teaser.gif" alt=""></div></div></li><li class="mt-4"><div class="row"><div class="col-8">A Distributed Hardware Prototype Targeting Distributed Deep Learning for On-device Inference], Allen J Farcas (University of Texas at Austin); Radu Marculescu (University of Texas at Austin)*; Kartikeya Bhardwaj (); Guihong Li (UT)
                  </div><div class="col-4"><img style="width: 200px; height:auto" src="http://cvpr20.s3-website-us-west-2.amazonaws.com/CVPR20/W46/8/8-teaser.gif" alt=""></div></div></li></ol></td></tr><tr><td>07:10</td><td>QA, Moderator: Ming-Ching Chang, SUNY Albany </td></tr><tr><td>07:30</td><td>Invited Speeches:<ol><li>Yiran Chen, Duke, Finding a Sparse Neural Network Model</li><li>Philip Lapsley, Embedded Vision Alliance, Business Opportunities</li><li>Carole-Jean Wu, Facebook, MLPerf</li></ol></td></tr><tr><td>08:15</td><td>QA, Moderator: Carole-Jean Wu, Facebook </td></tr><tr><td>08:30</td><td>Invited Speeches:<br><ol><li>Evgeni Gousev, Qualcomm, TinyML</li><li>Vikas Chandra, Facebook </li><li>Song Han, MIT: Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms</li></ol></td></tr><tr><td>09:15</td><td>QA, Moderator: Jaeyoun Kim, Google </td></tr><tr><td>09:30</td><td>Invited Speech (Tools): <br><ol><li>Christian Keller, Facebook</li><li>Ashish Sirasao, Xilinx</li><li>Bo Chen, Google</li></ol></td></tr><tr><td>10:15</td><td>QA, Moderator: Joe Spisak, Facebook </td></tr><tr><td>10:30</td><td>Panel: How can you build startups using low-power computer vision? Moderator: Song Han, MIT <p>Pnaelists:</p><ol><li>Philip Lapsley, Embedded Vision Alliance, Business Opportunities</li><li>Evgeni Gousev, Qualcomm, TinyML. </li><li>Carole-Jean Wu, Facebook, MLPerf</li><li>Kurt Keutzer, Berkeley </li><li>Mohammad Rastegari, Apple</li></ol></td></tr><tr><td>11:30</td><td>Adjourn</td></tr></tbody></table>
          <!--==========================
          Organizers Section
          ============================-->
          
          <h3>Organizers</h3>
          <table>
            <thead>
              <tr>
                <th width="20%;">Name</th>
                <th width="70%;">Role and Organization</th>
                <th width="10%;">Email</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>Hartwig Adam</td><td>Google</td><td><a href="mailto:hadam@google.com">hadam@google.com</a></td></tr>
              <tr><td>Mohamed M. Sabry Aly</td><td>IEEE Council on Design Automation, Nanyang Technolgical University</td><td><a href="mailto:msabry@ntu.edu.sg">msabry@ntu.edu.sg</a></td></tr>
              <tr><td>Alexander C Berg</td><td>University of North Carolina</td><td><a href="mailto:aberg@cs.unc.edu">aberg@cs.unc.edu</a></td></tr>
              <tr><td>Ming-Ching Chang</td><td>University at Albany - SUNY</td><td><a href="mailto:mchang2@albany.edu">mchang2@albany.edu</a></td></tr>
              <tr><td>Bo Chen</td><td>Google</td><td><a href="mailto:bochen@google.com">bochen@google.com</a></td></tr>
              <tr><td>Shu-Ching Chen</td><td>Florida International University</td><td><a href="mailto:chens@cs.fiu.edu">chens@cs.fiu.edu</a></td></tr>
              <tr><td>Yen-Kuang Chen</td><td>IEEE Circuits and Systems Society, Alibaba</td><td><a href="mailto:y.k.chen@ieee.org">y.k.chen@ieee.org</a></td></tr>
              <tr><td>Yiran Chen</td><td>ACM Speical Interest Group on Design Automation, Duke University</td><td><a href="mailto:yiran.chen@duke.edu">yiran.chen@duke.edu</a></td></tr>
              <tr><td>Chia-Ming Cheng</td><td>Mediatek</td><td><a href="mailto:cm.cheng@mediatek.com">cm.cheng@mediatek.com</a></td></tr>
              <tr><td>Bohyung Han</td><td>Seoul National University</td><td><a href="mailto:bhhan@snu.ac.kr">bhhan@snu.ac.kr</a></td></tr>
              <tr><td>Andrew Howard</td><td>Google</td><td><a href="mailto:howarda@google.com">howarda@google.com</a></td></tr>
              <tr><td>Xiao Hu</td><td>manage lpcv.ai website, Purdue student</td><td><a href="mailto:hu440@purdue.edu">hu440@purdue.edu</a></td></tr>
              <tr><td>Christian Keller</td><td>Facebook</td><td><a href="mailto:chk@fb.com">chk@fb.com</a></td></tr>
              <tr><td>Jaeyoun Kim</td><td>Google</td><td><a href="mailto:jaeyounkim@google.com">jaeyounkim@google.com</a></td></tr>
              <tr><td>Mark Liao</td><td>Academia Sinica, Taiwan</td><td><a href="mailto:liao@iis.sinica.edu.tw">liao@iis.sinica.edu.tw</a></td></tr>
              <tr><td>Tsung-Han Lin</td><td>Mediatek</td><td><a href="mailto:Tsung-Han.Lin@mediatek.com">Tsung-Han.Lin@mediatek.com</a></td></tr>
              <tr><td><b>(contact)</b> Yung-Hsiang Lu</td><td>Chair, Purdue</td><td><a href="mailto:yunglu@purdue.edu">yunglu@purdue.edu</a></td></tr>
              <tr><td>An Luo</td><td>Amazon</td><td><a href="mailto:anluo@amazon.com">anluo@amazon.com</a></td></tr>
              <tr><td>Terence Martinez</td><td>IEEE Future Directions</td><td><a href="mailto: t.c.martinez@ieee.org"> t.c.martinez@ieee.org</a></td></tr>
              <tr><td>Ryan McBee</td><td>Southwest Research Institute</td><td><a href="mailto:ryan.mcbee@swri.org">ryan.mcbee@swri.org</a></td></tr>
              <tr><td>Andreas Moshovos</td><td>University of Toronto/Vector Institute</td><td><a href="mailto:moshovos@ece.utoronto.ca">moshovos@ece.utoronto.ca</a></td></tr>
              <tr><td>Naveen Purushotham</td><td>Xilinx Inc.</td><td><a href="mailto:npurusho@xilinx.com">npurusho@xilinx.com</a></td></tr>
              <tr><td>Tim Rogers</td><td>Purdue</td><td><a href="mailto:timrogers@purdue.edu">timrogers@purdue.edu</a></td></tr>
              <tr><td>Mei-Ling Shyu</td><td>IEEE Multimedia Technical Committee, Miami University</td><td><a href="mailto:shyu@miami.edu">shyu@miami.edu</a></td></tr>
              <tr><td>Ashish Sirasao</td><td>Xilinx Inc.</td><td><a href="mailto:asirasa@xilinx.com">asirasa@xilinx.com</a></td></tr>
              <tr><td>Joseph Spisak</td><td>Facebook</td><td><a href="mailto:jspisak@fb.com">jspisak@fb.com</a></td></tr>
              <tr><td>George K. Thiruvathukal</td><td>Loyola University Chiicago</td><td><a href="mailto:gkt@cs.luc.edu">gkt@cs.luc.edu</a></td></tr>
              <tr><td>Yueh-Hua Wu</td><td>Academia Sinica, Taiwan</td><td><a href="mailto:kriswu@iis.sinica.edu.tw">kriswu@iis.sinica.edu.tw</a></td></tr>
              <tr><td>Junsong Yuan</td><td>University at Buffalo</td><td><a href="mailto:jsyuan@buffalo.edu">jsyuan@buffalo.edu</a></td></tr>
              <tr><td>Anirudh Vegesana</td><td>manage lpcv.ai website, Purdue student</td><td><a href="mailto:avegesan@purdue.edu">avegesan@purdue.edu</a></td></tr>
              <tr><td>Carole-Jean Wu</td><td>Facebook</td><td><a href="mailto:carolejeanwu@fb.com">carolejeanwu@fb.com</a></td></tr>
            </tbody>
            </table>
            <!--=======================
            FAQ
            ========================-->
            <hr>
            <h2><i class="fa fa-question mr-2"></i>FAQ</h2>
            <ol><!-- Follow this format of posting Q&A
              <li><p class="mb-3"></p></li>
              --><li>Q: Where can I sign the agreement form?
                <p class="mb-3">A: You should sign the agreement form when submitting your solution to any track. Each account only needs to sign it once. The agreement form is a legal binding document, so please read it carefully.</p></li><li>Q: How can we register as a team?
                <p class="mb-3">A: One team only needs one registered account. You can indicate your team members when signing the agreement form.</p></li><li>Q: How can I register for a specific track?
                <p class="mb-3">A: Registered accounts are not limited to attend any track. Accounts will be considered as participating a track when submitting a solution.</p></li></ol>
    </div>
</div>
<div class="clearfix"></div></div></article>
{% endblock %}